{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a59a5cf-c60c-4c65-95fb-4c1e85ed3185",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "\n",
    "import logging \n",
    "import sys, os, re\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "import pickle\n",
    "import conllu\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from topictuner import TopicModelTuner\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "from gensim.matutils import hellinger\n",
    "from gensim.matutils import kullback_leibler\n",
    "from gensim.matutils import jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc658e3a-cc20-41d9-9dbd-45ae49378e1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# util classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "453d42c3-87d6-402f-9ee5-1a13840a7834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###############\n",
    "#These classes for UD object types (treebank, sentence, word)\n",
    "#are from https://github.com/personads/ud-selection/blob/main/lib/data.py\n",
    "###############\n",
    "class UniversalDependencies:\n",
    "\n",
    "    def __init__(self, treebanks=[]):\n",
    "        self._treebanks = treebanks\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_directory(path, verbose=False):\n",
    "        treebanks = []\n",
    "        cursor = 0\n",
    "            \n",
    "        # iterate over files in TB directory\n",
    "        for tbf in sorted(os.listdir(path)):\n",
    "            tbf_path = os.path.join(path, tbf)\n",
    "            \n",
    "           \n",
    "            # parse TB sample name with lang fam and GMM/gold\n",
    "            r = r'(.+)(?=(\\.dev|\\.train)\\.conllu)'\n",
    "            tb_match = re.match(r, tbf)\n",
    "            if not tb_match:\n",
    "                continue\n",
    "            fname = tb_match.group(0)\n",
    "            # print(f'fname: {fname}\\n')\n",
    "                      \n",
    "            components = fname.split('__')\n",
    "            # print(f'components: {components}\\n')\n",
    "            \n",
    "            tb_meta = defaultdict()\n",
    "            \n",
    "            if ('dev' in tbf_path)&(len(components) == 3):\n",
    "                # tb_meta['Language_family'] = components[0]\n",
    "                tb_meta['Language'] = components[0]\n",
    "                tb_meta['Treebank'] = components[1]\n",
    "                tb_meta['Genre'] = components[2]\n",
    "                \n",
    "            \n",
    "            elif 'train' in tbf_path:\n",
    "                # tb_meta['Language_family'] = components[0]\n",
    "                # tb_meta['Sample_type'] = components[1]\n",
    "                tb_meta['Language'] = components[0]\n",
    "                tb_meta['Treebank'] = components[1]\n",
    "                tb_meta['Genre'] = components[2]\n",
    "                tb_meta['Sample_no'] = components[3]\n",
    "                tb_meta['Seed'] = components[4]\n",
    "            print(tb_meta)    \n",
    "            \n",
    "            # skip non-conllu files\n",
    "            if os.path.splitext(tbf)[1] != '.conllu': continue\n",
    "\n",
    "            # load treebank\n",
    "            treebank = UniversalDependenciesTreebank.from_conllu(tbf_path, name=tbf, meta=tb_meta, start_idx=cursor, ud_filter=None)\n",
    "            treebanks.append(treebank)\n",
    "            cursor += len(treebank)\n",
    "\n",
    "            # print statistics (if verbose)\n",
    "            if verbose:\n",
    "                info = f\"Loaded {treebank}.\"\n",
    "            if logging.getLogger().hasHandlers():\n",
    "                logging.info(info)\n",
    "            else:\n",
    "                print(info)\n",
    "\n",
    "        return UniversalDependencies(treebanks=treebanks)\n",
    "\n",
    "    def get_treebanks(self):\n",
    "        return self._treebanks\n",
    "\n",
    "class UniversalDependenciesTreebank:\n",
    "\tdef __init__(self, sentences=[], name=None, meta={}):\n",
    "\t\tself._sentences = sentences\n",
    "\t\tself._name = name\n",
    "\t\tself._meta = meta\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn f'<UniversalDependenciesTreebank{f\" ({self._name})\" if self._name else \"\"}: {len(self._sentences)} sentences>'\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self._sentences)\n",
    "\n",
    "\tdef __getitem__(self, key):\n",
    "\t\treturn self._sentences[key]\n",
    "\n",
    "\tdef __setitem__(self, key, val):\n",
    "\t\tself._sentences[key] = val\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef from_conllu(path, name=None, meta=None, start_idx=0, ud_filter=None):\n",
    "\t\tsentences = []\n",
    "\t\twith open(path, 'r', encoding='utf8') as fp:\n",
    "\t\t\tcur_lines = []\n",
    "\t\t\tfor line_idx, line in enumerate(fp):\n",
    "\t\t\t\tline = line.strip()\n",
    "\t\t\t\t# on blank line, construct full sentence from preceding lines\n",
    "\t\t\t\tif line == '':\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t# parse sentence from current set of lines\n",
    "\t\t\t\t\t\tsentence = UniversalDependenciesSentence.from_conllu(start_idx + len(sentences), cur_lines)\n",
    "\t\t\t\t\t\t# if filter is set, set any sentences not matching the filter to None\n",
    "\t\t\t\t\t\tif (ud_filter is not None) and (not ud_filter(sentence, meta)): sentence = None\n",
    "\t\t\t\t\t\t# append sentence to results\n",
    "\t\t\t\t\t\tsentences.append(sentence)\n",
    "\t\t\t\t\texcept Exception as err:\n",
    "\t\t\t\t\t\twarn_msg = f\"[Warning] UniversalDependenciesTreebank: Unable to parse '{path}' line {line_idx} ({err}). Skipping.\"\n",
    "\t\t\t\t\t\tif logging.getLogger().hasHandlers():\n",
    "\t\t\t\t\t\t\tlogging.warning(warn_msg)\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tprint(warn_msg)\n",
    "\t\t\t\t\tcur_lines = []\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tcur_lines.append(line)\n",
    "\t\treturn UniversalDependenciesTreebank(sentences=sentences, name=name, meta=meta)\n",
    "\n",
    "\tdef to_tokens(self):\n",
    "\t\tsentences = []\n",
    "\t\tfor sentence in self:\n",
    "\t\t\tsentences.append(sentence.to_tokens())\n",
    "\t\treturn sentences\n",
    "\n",
    "\tdef to_words(self):\n",
    "\t\tsentences = []\n",
    "\t\tfor sentence in self:\n",
    "\t\t\tsentences.append(sentence.to_words())\n",
    "\t\treturn sentences\n",
    "\n",
    "\tdef to_conllu(self, comments=True, resolve=False):\n",
    "\t\treturn ''.join([s.to_conllu(comments, resolve) for s in self._sentences])\n",
    "\n",
    "\tdef get_sentences(self):\n",
    "\t\treturn self._sentences\n",
    "\n",
    "\tdef get_name(self):\n",
    "\t\treturn self._name\n",
    "\n",
    "\tdef get_treebank_name(self):\n",
    "\t\treturn self._meta.get('Treebank', 'Unknown')\n",
    "\n",
    "\tdef get_language(self):\n",
    "\t\treturn self._meta.get('Language', 'Unknown')\n",
    "\n",
    "\tdef get_domains(self):\n",
    "\t\treturn sorted(self._meta.get('Genre', '').split(' '))\n",
    "\n",
    "\tdef get_statistics(self):\n",
    "\t\tstatistics = {\n",
    "\t\t\t'sentences': len(self._sentences),\n",
    "\t\t\t'tokens': 0,\n",
    "\t\t\t'words': 0,\n",
    "\t\t\t'metadata': set()\n",
    "\t\t}\n",
    "\n",
    "\t\tfor sidx, sentence in enumerate(self):\n",
    "\t\t\tstatistics['tokens'] += len(sentence.to_tokens(as_str=False))\n",
    "\t\t\tstatistics['words'] += len(sentence.to_words(as_str=False))\n",
    "\t\t\tstatistics['metadata'] |= set(sentence.get_metadata().keys())\n",
    "\n",
    "\t\tstatistics['metadata'] = list(sorted(statistics['metadata']))\n",
    "\n",
    "\t\treturn statistics\n",
    "\n",
    "    \n",
    "class UniversalDependenciesSentence:\n",
    "\tdef __init__(self, idx, tokens, comments=[]):\n",
    "\t\tself.idx = idx\n",
    "\t\tself._tokens = tokens\n",
    "\t\tself._comments = comments\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn f\"<UniversalDependenciesSentence: ID {self.idx}, {len(self._tokens)} tokens, {len(self._comments)} comments>\"\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef from_conllu(idx, lines):\n",
    "\t\ttokens, comments = [], []\n",
    "\t\tline_idx = 0\n",
    "\t\twhile line_idx < len(lines):\n",
    "\t\t\t# check for comment\n",
    "\t\t\tif lines[line_idx].startswith('#'):\n",
    "\t\t\t\tcomments.append(lines[line_idx])\n",
    "\t\t\t\tline_idx += 1\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t# process tokens\n",
    "\t\t\ttkn_words = []\n",
    "\t\t\ttkn_line_split = lines[line_idx].split('\\t')\n",
    "\t\t\ttkn_idx_str = tkn_line_split[0]\n",
    "\t\t\t# check for multiword token in 'a-b' format\n",
    "\t\t\tnum_words = 1\n",
    "\t\t\tif '-' in tkn_idx_str:\n",
    "\t\t\t\ttkn_idx_split = tkn_idx_str.split('-')\n",
    "\t\t\t\t# convert token id to tuple signifying range (e.g. (3,4))\n",
    "\t\t\t\ttkn_span = (int(tkn_idx_split[0]), int(tkn_idx_split[1]))\n",
    "\t\t\t\t# collect the number of words in the current span\n",
    "\t\t\t\twhile (line_idx + num_words + 1) < len(lines):\n",
    "\t\t\t\t\tnum_words += 1\n",
    "\t\t\t\t\t# get current index as float due to spans such as '1-2; 1; 2; 2.1; ... 3' (e.g. Arabic data)\n",
    "\t\t\t\t\tspan_str = lines[line_idx+num_words].split('\\t')[0]\n",
    "\t\t\t\t\tif '-' in span_str: break\n",
    "\t\t\t\t\tspan_tkn_idx = float(span_str)\n",
    "\t\t\t\t\tif int(span_tkn_idx) > tkn_span[1]: break\n",
    "\t\t\t# check for multiword token in decimal format '1; 1.1; 1.2; ... 2' or '0.1; 0.2; ... 1' (e.g. Czech data)\n",
    "\t\t\telif re.match(r'^\\d+\\.\\d+', tkn_idx_str)\t\t\t\tor ((line_idx < (len(lines) - 1)) and re.match(r'^\\d+\\.\\d+\\t', lines[line_idx+1])):\n",
    "\t\t\t\t# count words that are part of multiword token\n",
    "\t\t\t\twhile (line_idx + num_words) < len(lines):\n",
    "\t\t\t\t\tif not re.match(r'^\\d+\\.\\d+\\t', lines[line_idx+num_words]):\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\tnum_words += 1\n",
    "\t\t\t\t# token span for decimal indices is (a.1, a.n)\n",
    "\t\t\t\ttkn_span_start = float(tkn_idx_str) if re.match(r'^\\d+\\.\\d+', tkn_idx_str) else int(tkn_idx_str) + .1\n",
    "\t\t\t\ttkn_span_end = tkn_span_start + (.1 * (num_words - 1))\n",
    "\t\t\t\ttkn_span = (tkn_span_start, tkn_span_end)\n",
    "\t\t\t# if single word token\n",
    "\t\t\telse:\n",
    "\t\t\t\t# convert token id to tuple with range 1 (e.g. (3,3))\n",
    "\t\t\t\ttkn_span = (int(tkn_idx_str), int(tkn_idx_str))\n",
    "\n",
    "\t\t\t# construct words contained in token\n",
    "\t\t\tfor word_line in lines[line_idx:line_idx + num_words]:\n",
    "\t\t\t\ttkn_words.append(UniversalDependenciesWord.from_conllu(word_line))\n",
    "\t\t\t# construct and append token\n",
    "\t\t\ttokens.append(UniversalDependenciesToken(idx=tkn_span, words=tkn_words))\n",
    "\t\t\t# increment line index by number of words in token\n",
    "\t\t\tline_idx += num_words\n",
    "\n",
    "\t\treturn UniversalDependenciesSentence(idx=idx, tokens=tokens, comments=comments)\n",
    "\n",
    "\tdef to_text(self):\n",
    "\t\treturn ''.join([t.to_text() for t in self._tokens])\n",
    "\n",
    "\tdef to_tokens(self, as_str=True):\n",
    "\t\treturn [(t.get_form() if as_str else t) for t in self._tokens]\n",
    "\n",
    "\tdef to_words(self, as_str=True):\n",
    "\t\treturn [(w.get_form() if as_str else w) for token in self._tokens for w in token.to_words()]\n",
    "\n",
    "\tdef to_conllu(self, comments=True, resolve=False):\n",
    "\t\tconllu = '\\n'.join(self._comments) + '\\n' if comments else ''\n",
    "\n",
    "\t\tconllu += '\\n'.join([t.to_conllu(resolve=resolve) for t in self._tokens])\n",
    "\t\tconllu += '\\n\\n'\n",
    "\n",
    "\t\treturn conllu\n",
    "\n",
    "\tdef get_dependencies(self, offset=-1, include_subtypes=True):\n",
    "\n",
    "\t\tlabels = []\n",
    "\t\theads = []\n",
    "\t\tfor token in self._tokens:\n",
    "\t\t\tfor w in token.to_words():\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\theads.append((w.head + offset))\n",
    "\t\t\t\texcept TypeError:\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\n",
    "\t\tfor token in self._tokens:\n",
    "\t\t\tfor w in token.to_words():\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tif include_subtypes:\n",
    "\t\t\t\t\t\tlabels.append(w.deprel)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tw.deprel.split(':')[0]\n",
    "\t\t\t\texcept TypeError:\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\tif labels[0] == None:       \n",
    "\t\t\tlabels.pop(0)\n",
    "\t\t# heads = [\n",
    "\t\t# \t(w.head + offset)\n",
    "\t\t# \tfor token in self._tokens for w in token.to_words()\n",
    "\t\t# ]\n",
    "\t\t# labels = [\n",
    "\t\t# \tw.deprel if include_subtypes else w.deprel.split(':')[0]\n",
    "\t\t# \tfor token in self._tokens for w in token.to_words()\n",
    "\t\t# ]\n",
    "\t\treturn heads, labels\n",
    "\n",
    "\tdef get_comments(self, stripped=True):\n",
    "\t\treturn [c[1:].strip() for c in self._comments]\n",
    "\n",
    "\tdef get_metadata(self):\n",
    "\t\t\"\"\"Returns metadata from the comments of a sentence.\n",
    "\n",
    "\t\tComment should follow the UD metadata guidelines '# FIELD = VALUE' or '# FIELD VALUE.\n",
    "\t\tLines not following this convention are exported in the 'unknown' field.\n",
    "\n",
    "\t\tReturns a dict of metadata field and value pairs {'FIELD': 'VALUE'}.\n",
    "\t\t\"\"\"\n",
    "\t\tmetadata = {}\n",
    "\t\tmd_patterns = [r'^# ?(.+?) ?= ?(.+)', r'^# ?([^\\s]+?)\\s([^\\s]+)$']\n",
    "\t\tfor comment in self._comments:\n",
    "\t\t\tfor md_pattern in md_patterns:\n",
    "\t\t\t\tmd_match = re.match(md_pattern, comment)\n",
    "\t\t\t\tif md_match:\n",
    "\t\t\t\t\tmetadata[md_match[1]] = md_match[2]\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\telse:\n",
    "\t\t\t\tmetadata['unknown'] = metadata.get('unknown', []) + [comment[1:].strip()]\n",
    "\t\treturn metadata\n",
    "\n",
    "\n",
    "class UniversalDependenciesToken:\n",
    "\tdef __init__(self, idx, words):\n",
    "\t\tself.idx = idx # expects int or float tuple\n",
    "\t\tself._words = words # first element is token form, all following belong to potential multiword tokens\n",
    "\n",
    "\tdef to_text(self):\n",
    "\t\treturn self._words[0].to_text()\n",
    "\n",
    "\tdef to_words(self):\n",
    "\t\t# if single word token\n",
    "\t\tif len(self._words) == 1:\n",
    "\t\t\treturn self._words\n",
    "\t\t# if multiword token\n",
    "\t\telse:\n",
    "\t\t\t# return words which have a dependency head\n",
    "\t\t\treturn [w for w in self._words if w.head is not None]\n",
    "\n",
    "\tdef to_conllu(self, resolve=False):\n",
    "\t\t# resolve multiword tokens into its constituents\n",
    "\t\tif resolve:\n",
    "\t\t\t# if form token has no head (e.g. 'i-j' token), get constituent words\n",
    "\t\t\tif (self._words[0].head is None) and (len(self._words) > 1):\n",
    "\t\t\t\treturn '\\n'.join([w.to_conllu() for w in self._words[1:] if w.head is not None])\n",
    "\t\t\t# if form token has head or it is not a multiword token, return itself\n",
    "\t\t\telif self._words[0].head is not None:\n",
    "\t\t\t\treturn self._words[0].to_conllu()\n",
    "\t\t\t# if token consists of only one word which has no head, omit (e.g. Coptic '0.1')\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn ''\n",
    "\t\t# otherwise return full set of words\n",
    "\t\telse:\n",
    "\t\t\treturn '\\n'.join([w.to_conllu() for w in self._words])\n",
    "\n",
    "\tdef get_form(self):\n",
    "\t\treturn self._words[0].get_form()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "class UniversalDependenciesWord:\n",
    "\t\"\"\"\n",
    "\tID: Word index, integer starting at 1 for each new sentence; may be a range for tokens with multiple words.\n",
    "\tFORM: Word form or punctuation symbol.\n",
    "\tLEMMA: Lemma or stem of word form.\n",
    "\tUPOSTAG: Universal part-of-speech tag drawn from our revised version of the Google universal POS tags.\n",
    "\tXPOSTAG: Language-specific part-of-speech tag; underscore if not available.\n",
    "\tFEATS: List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available.\n",
    "\tHEAD: Head of the current token, which is either a value of ID or zero (0).\n",
    "\tDEPREL: Universal Stanford dependency relation to the HEAD (root iff HEAD = 0) or a defined language-specific subtype of one.\n",
    "\tDEPS: List of secondary dependencies (head-deprel pairs).\n",
    "\tMISC: Any other annotation.\n",
    "\n",
    "\t[1] https://universaldependencies.org/docs/format.html\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, idx, form, lemma, upostag, xpostag, feats, head, deprel, deps=None, misc=None):\n",
    "\t\tself.idx = idx # expects int, float or str\n",
    "\t\tself.form = form\n",
    "\t\tself.lemma = lemma\n",
    "\t\tself.upostag = upostag\n",
    "\t\tself.xpostag = xpostag\n",
    "\t\tself.feats = feats # expects dict\n",
    "\t\tself.head = head # expects int\n",
    "\t\tself.deprel = deprel # expects str\n",
    "\t\tself.deps = deps\n",
    "\t\tself.misc = misc # expects dict\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn f'<UniversalDependenciesWord: ID {self.idx}, \"{self.form}\">'\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef from_conllu(line):\n",
    "\t\t# split line and initially convert '_' values to None\n",
    "\t\tidx_str, form, lemma, upostag, xpostag, feats, head, deprel, deps, misc = [(v if v != '_' else None) for v in line.split('\\t')]\n",
    "\t\t# parse idx string (int 1, decimal 1.1 or string '1-2')\n",
    "\t\tidx = idx_str\n",
    "\t\tif re.match(r'^\\d+\\.\\d+$', idx_str): idx = float(idx_str)\n",
    "\t\telif re.match(r'^\\d+$', idx_str): idx = int(idx_str)\n",
    "\t\t# parse form and lemma (special case '_')\n",
    "\t\tform = form if form is not None else '_'\n",
    "\t\tlemma = lemma if form != '_' else '_'\n",
    "\t\t# parse dependency head idx (int)\n",
    "\t\thead = int(head) if head is not None else head\n",
    "\t\t# parse FEATS dictionaries\n",
    "\t\ttry:\n",
    "\t\t\tfeats = {f.split('=')[0]:f.split('=')[1] for f in feats.split('|')}\n",
    "\t\texcept:\n",
    "\t\t\tfeats = {}\n",
    "\t\t# parse MISC dictionary\n",
    "\t\ttry:\n",
    "\t\t\tmisc = {m.split('=')[0]:m.split('=')[1] for m in misc.split('|')}\n",
    "\t\texcept:\n",
    "\t\t\tmisc = {}\n",
    "\t\t# construct word\n",
    "\t\tword = UniversalDependenciesWord(\n",
    "\t\t\tidx,\n",
    "\t\t\tform, lemma, # form and lemma are str\n",
    "\t\t\tupostag, xpostag, # upostag and xpostag are str\n",
    "\t\t\tfeats,\n",
    "\t\t\thead, deprel, deps, # dependency information as str\n",
    "\t\t\tmisc\n",
    "\t\t)\n",
    "\t\treturn word\n",
    "\n",
    "\tdef to_text(self):\n",
    "\t\ttext = self.get_form() + ' ' # form + space by default\n",
    "\t\t# if 'SpaceAfter=No' remove trailing space\n",
    "\t\tif ('SpaceAfter' in self.misc) and (self.misc['SpaceAfter'] == 'No'):\n",
    "\t\t\ttext = text[:-1]\n",
    "\n",
    "\t\treturn text\n",
    "\n",
    "\tdef to_conllu(self):\n",
    "\t\tconllu = ''\n",
    "\n",
    "\t\t# convert dictionaries\n",
    "\t\tfeats_str = '|'.join([f'{k}={v}' for k, v in sorted(self.feats.items())]) if self.feats else None\n",
    "\t\tmisc_str = '|'.join([f'{k}={v}' for k, v in sorted(self.misc.items())]) if self.misc else None\n",
    "\n",
    "\t\tconllu_values = [\n",
    "\t\t\tstr(self.idx),\n",
    "\t\t\tself.form, self.lemma,\n",
    "\t\t\tself.upostag, self.xpostag,\n",
    "\t\t\tfeats_str,\n",
    "\t\t\tstr(self.head), self.deprel, self.deps,\n",
    "\t\t\tmisc_str\n",
    "\t\t]\n",
    "\t\t# convert None to '_'\n",
    "\t\tconllu_values = [v if v is not None else '_' for v in conllu_values]\n",
    "\n",
    "\t\tconllu = '\\t'.join(conllu_values)\n",
    "\t\treturn conllu\n",
    "\n",
    "\tdef get_form(self):\n",
    "\t\tform = self.form if self.form else ''\n",
    "\t\tform = form.replace('\\xad', '') # sanitize soft hyphens\n",
    "\t\treturn form\n",
    "#initialize comfy functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e9f408-65db-4022-9bfd-c6305308cf52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "# basic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fbe25de-fe89-4e0e-befb-63adef28c7b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "def clean_sentences(sents):\n",
    "        \n",
    "    clean_sents = []\n",
    "    for sent in sents:\n",
    "        sent = re.sub(r'<a_href=\\S+', '', sent)\n",
    "        sent = re.sub(r'<(/)?\\w+>', '', sent)\n",
    "        sent = re.sub(\"[#*]\", \"\", sent)\n",
    "        sent = remove_emoji(sent)\n",
    "        clean_sents.append(sent)\n",
    "        \n",
    "    return clean_sents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743df0de-5111-4ba7-8117-6162813e523d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# BERTOPIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49e4917b-cacf-4f14-92fe-f45422f01e26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class bertopicTM():\n",
    "    \n",
    "    def __init__(self, preprocessed_sentences):\n",
    "        \n",
    "        self.docs = preprocessed_sentences\n",
    "        self.emb_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "        # self.emb_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
    "        \n",
    "        # self.emb_model = SentenceTransformer(\"intfloat/multilingual-e5-base\")\n",
    "        # self.docs = [f'query: {text}' for text in preprocessed_sentences] # multilingual-e5-large requirement  \n",
    "        \n",
    "        self.embeddings : np.ndarray() = None\n",
    "        self.umap_model = UMAP(\n",
    "            n_neighbors=15,\n",
    "            n_components=5,\n",
    "            min_dist=0.0,\n",
    "            metric='cosine',\n",
    "            # low_memory=False,\n",
    "            random_state=42)\n",
    "        self.hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True) #try leaf, eom tends to create big clus\n",
    "        \n",
    "        #tune hdbscan\n",
    "        self.tmt = TopicModelTuner(docs = self.docs, embedding_model = self.emb_model, hdbscan_model=self.hdbscan_model, reducer_model=self.umap_model, reducer_random_state=1337, verbose = 2)\n",
    "        self.BestResultsDF_random = pd.DataFrame()\n",
    "        self.BestResultsDF_pseudoG = pd.DataFrame()\n",
    "        self.BestResultsDF_grid = pd.DataFrame()\n",
    "        \n",
    "        #topic representation\n",
    "        self.vectorizer_model = CountVectorizer(max_df = 0.7, ngram_range = (1,2))\n",
    "        self.ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True, bm25_weighting=True)#bm25_weighting=True - robustness to stopwords\n",
    "        \n",
    "        keybert_model = KeyBERTInspired()\n",
    "        mmr_model = MaximalMarginalRelevance(diversity=0.5)\n",
    "        \n",
    "        self.representation_model = {\n",
    "            \"KeyBERT\": keybert_model,\n",
    "            # \"OpenAI\": openai_model,  # Uncomment if you will use OpenAI\n",
    "            \"MMR\": mmr_model,\n",
    "        }\n",
    "        \n",
    "\n",
    "    def get_range_check(self):\n",
    "\n",
    "        best_num_clusters = max(self.tmt.ResultsDF.sort_values(by = 'number_uncategorized').query('number_uncategorized < 1000').number_of_clusters.tolist())\n",
    "        best_clu_size = self.tmt.ResultsDF.sort_values(by = 'number_uncategorized').query('number_uncategorized < 1000 & number_of_clusters == @best_num_clusters').min_cluster_size.tolist()[0]\n",
    "        if best_clu_size < 30:\n",
    "            perc = 0.5\n",
    "        else:\n",
    "            perc = 0.1\n",
    "        \n",
    "        lower_limit = best_clu_size - int(best_clu_size*perc)\n",
    "        if lower_limit == 1:\n",
    "            lower_limit = 2\n",
    "        higher_limit = best_clu_size + int(best_clu_size*perc)\n",
    "        range_check = [*range(lower_limit,higher_limit)]\n",
    "\n",
    "        return range_check \n",
    "    \n",
    "    def tune_hdbscan(self):\n",
    "        \n",
    "        self.embeddings = self.tmt.createEmbeddings(self.docs)\n",
    "        self.tmt.reduce()\n",
    "        self.tmt.randomSearch([*range(10,15)], [.1, .25, .5, .75, 1], iters = 50)\n",
    "        self.BestResultsDF_random = self.tmt.summarizeResults()\n",
    "        self.BestResultsDF_pseudoG = self.tmt.pseudoGridSearch(self.get_range_check(), [x/100 for x in range(10,101,10)])       \n",
    "        self.BestResultsDF_grid = self.tmt.gridSearch(self.get_range_check())\n",
    "        self.min_clu_size = self.BestResultsDF_grid.query('number_uncategorized == number_uncategorized.min()').min_cluster_size.tolist()[0]\n",
    "        self.min_samples = self.BestResultsDF_grid.query('number_uncategorized == number_uncategorized.min()').min_samples.tolist()[0]#\n",
    "          \n",
    "\n",
    "    def run_TM(self, use_tuning = False, reduce_outliers = False):\n",
    "\n",
    "        if not use_tuning:\n",
    "\n",
    "            hdbscan_model=self.hdbscan_model\n",
    "        \n",
    "        else:\n",
    "\n",
    "            self.tune_hdbscan()\n",
    "\n",
    "            hdbscan_model = HDBSCAN(min_cluster_size=self.min_clu_size, min_samples = self.min_samples, metric='euclidean', cluster_selection_method='eom', prediction_data=True) \n",
    "\n",
    "        # All steps together\n",
    "        model = BERTopic(\n",
    "              embedding_model=self.emb_model,          # Step 1 - Extract embeddings\n",
    "              umap_model=self.umap_model,                    # Step 2 - Reduce dimensionality\n",
    "              hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings\n",
    "              vectorizer_model=self.vectorizer_model,        # Step 4 - Tokenize topics\n",
    "              ctfidf_model=self.ctfidf_model,                # Step 5 - Extract topic words\n",
    "              representation_model=self.representation_model, # Step 6 - (Optional) Fine-tune topic represenations\n",
    "              calculate_probabilities = True,\n",
    "              # top_n_words = 15\n",
    "            )\n",
    "        \n",
    "        self.model = model.fit(self.docs, self.embeddings)\n",
    "        # topics, probs = self.model.transform(self.docs, self.embeddings)\n",
    "        \n",
    "        if reduce_outliers:\n",
    "            \n",
    "            topics_, _ = self.model.transform(self.docs, self.embeddings)\n",
    "            new_topics = self.model.reduce_outliers(self.docs, topics_, strategy=\"embeddings\")\n",
    "            self.model.update_topics(self.docs, topics=new_topics)\n",
    "            \n",
    "            # new_topics = self.model.reduce_outliers(self.docs, self.model.topics_, strategy=\"embeddings\", threshold = 0.5)\n",
    "            # self.model.update_topics(self.docs, topics=new_topics, top_n_words = 15, vectorizer_model=self.vectorizer_model, ctfidf_model=self.ctfidf_model)\n",
    "                          \n",
    "        return self.model\n",
    "        \n",
    "    \n",
    "    def _calculate_topic_diversity(self):\n",
    "        \n",
    "        topic_keywords = self.model.get_topics()\n",
    "\n",
    "        bertopic_topics = []\n",
    "        for k,v in topic_keywords.items():\n",
    "            temp = []\n",
    "            for tup in v:\n",
    "                temp.append(tup[0])\n",
    "            bertopic_topics.append(temp)  \n",
    "\n",
    "        unique_words = set()\n",
    "        for topic in bertopic_topics:\n",
    "            unique_words = unique_words.union(set(topic[:10]))\n",
    "        td = len(unique_words) / (10 * len(bertopic_topics))\n",
    "\n",
    "        return td\n",
    "\n",
    "\n",
    "    def _calculate_cv_npmi(self, docs, topics): \n",
    "\n",
    "        doc = pd.DataFrame({\"Document\": docs,\n",
    "                        \"ID\": range(len(docs)),\n",
    "                        \"Topic\": topics})\n",
    "        documents_per_topic = doc.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "        cleaned_docs = self.model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "        vectorizer = self.model.vectorizer_model\n",
    "        analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "        words = vectorizer.get_feature_names_out()\n",
    "        tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "        dictionary = corpora.Dictionary(tokens)\n",
    "        corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "        topic_words = [[words for words, _ in self.model.get_topic(topic)] \n",
    "                    for topic in range(len(set(topics))-1)]\n",
    "\n",
    "        coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                      texts=tokens, \n",
    "                                      corpus=corpus,\n",
    "                                      dictionary=dictionary, \n",
    "                                      coherence='c_v')\n",
    "        cv_coherence = coherence_model.get_coherence()\n",
    "\n",
    "        coherence_model_npmi = CoherenceModel(topics=topic_words, \n",
    "                                      texts=tokens, \n",
    "                                      corpus=corpus,\n",
    "                                      dictionary=dictionary, \n",
    "                                      coherence='c_npmi')\n",
    "        npmi_coherence = coherence_model_npmi.get_coherence()\n",
    "\n",
    "        return cv_coherence, npmi_coherence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8be169db-c9e2-4615-a673-2cb976ad7e63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def substitute_vals(mean_probs, distance, len_topic_info):\n",
    "    \n",
    "    sorted_dict = {k: mean_probs[k] for k in sorted(mean_probs)}\n",
    "\n",
    "    # Drop key -1 and add keys to complete the range \n",
    "    for key in range(len_topic_info):\n",
    "        if key not in sorted_dict:\n",
    "            if distance == 'jaccard':\n",
    "                sorted_dict[key] = 0.0\n",
    "            else:\n",
    "                sorted_dict[key] = 0.00001\n",
    "                \n",
    "    sorted_dict = {k: sorted_dict[k] for k in sorted(sorted_dict)}\n",
    "    \n",
    "    # Substitute all 0.0 values with 0.0001\n",
    "    for key, value in sorted_dict.items():\n",
    "        if value == 0.0:\n",
    "            if distance == 'jaccard':\n",
    "                sorted_dict[key] = 0.0\n",
    "            else:\n",
    "                sorted_dict[key] = 0.00001       \n",
    "    # try:\n",
    "    #     sorted_dict.pop(-1)\n",
    "    # except:\n",
    "    #     \"NO -1 key\"\n",
    "    return sorted_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a9771b-8ab1-486b-b258-94333a924ce6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LREC-WS test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0990f321-63cb-472f-9848-f235ccf166c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = 'LREC_treebanks_test'\n",
    "\n",
    "distances = defaultdict(dict)\n",
    "\n",
    "for tb_dir in sorted(os.listdir(path)):\n",
    "        \n",
    "    ud_path = os.path.join(path, tb_dir)\n",
    "    \n",
    "    if not os.path.isdir(ud_path):\n",
    "        continue\n",
    "       \n",
    "    ud = UniversalDependencies.from_directory(ud_path, verbose=True)\n",
    "    \n",
    "    prev_id = 0    \n",
    "    lengths = defaultdict(dict)\n",
    "    for tb in ud.get_treebanks():\n",
    "        \n",
    "        tb_set_leng = len(tb.get_sentences())\n",
    "        \n",
    "        lengths[tb.get_name()] = {'s' : prev_id, \n",
    "                                  'e': prev_id+tb_set_leng}\n",
    "        prev_id += tb_set_leng\n",
    "            \n",
    "    ################\n",
    "    # Get all sentences in tb and preprocess basically\n",
    "    ################\n",
    "    tb_sents = []\n",
    "    for tb_set in ud.get_treebanks():\n",
    "        \n",
    "        tb_sents+=[s.to_text() for s in tb_set.get_sentences()]\n",
    "        \n",
    "        l = tb_set._meta['Language']\n",
    "        t = tb_set._meta['Treebank']\n",
    "    \n",
    "    tb_sents = list(tb_sents) \n",
    "    # assert len(tb_sents) == 20970 #1st tb\n",
    "    # tb_sents = list(set(tb_sents)) #unique\n",
    "    clean_docs = clean_sentences(tb_sents)\n",
    "    \n",
    "    ################\n",
    "    BT = bertopicTM(clean_docs)\n",
    "    tb_model = BT.run_TM(use_tuning = False, reduce_outliers = True)\n",
    "        \n",
    "    \n",
    "#     print(f'{l}_{t}_tb_model', tb_model.topic_sizes_)\n",
    "#     spectopic_indices = [n for n, tid in enumerate(tb_model.topics_) if tid == 0]\n",
    "#     print('tb_model_sample_sentences: ','\\n'.join([s for n, s in enumerate(clean_docs) if n in spectopic_indices][:20]))\n",
    "    \n",
    "    # if (l == 'English')&(t == 'GUM'):\n",
    "    #     tbank_model.save(f\"{l}_{t}_model\", serialization=\"safetensors\")\n",
    "    \n",
    "    results_dev = defaultdict(dict)\n",
    "\n",
    "    for k in lengths.keys():\n",
    "        if 'dev' in k:\n",
    "            \n",
    "            reg = re.match(r'^([a-zA-Z-]+)__([a-zA-Z-]+)__([a-zA-Z_-]+).dev.conllu', k)\n",
    "            l = reg.group(1)\n",
    "            cor = reg.group(2)\n",
    "            g = reg.group(3)\n",
    "            \n",
    "            topic_prop = {key: value / sum(Counter(tb_model.topics_[lengths[k]['s']: lengths[k]['e']]).values()) \n",
    "                          for key, value in Counter(tb_model.topics_[lengths[k]['s']: lengths[k]['e']]).items()}\n",
    "            #dict(sorted(topic_prop.items(), key=lambda item: item[1]))\n",
    "            results_dev[f'{l}__{cor}'][f'{g}'] = topic_prop\n",
    "    \n",
    "    results_train = defaultdict(dict)\n",
    "\n",
    "    for k in lengths.keys():\n",
    "        if 'train' in k:\n",
    "            \n",
    "            reg = re.match(r'^([a-zA-Z-]+)__([a-zA-Z-]+)__([a-zA-Z_-]+)__s(\\d)__rs(\\d+).train.conllu', k)\n",
    "            l = reg.group(1)\n",
    "            cor = reg.group(2)\n",
    "            g = reg.group(3)\n",
    "            samp = reg.group(4)\n",
    "            rseed = reg.group(5)\n",
    "            \n",
    "            topic_prop = {}\n",
    "            topic_prop = {key: value / sum(Counter(tb_model.topics_[lengths[k]['s']: lengths[k]['e']]).values())\n",
    "                          for key, value in Counter(tb_model.topics_[lengths[k]['s']: lengths[k]['e']]).items()}\n",
    "\n",
    "            results_train[f'{l}__{cor}'][f'{g}_{samp}_{rseed}'] = topic_prop\n",
    "    \n",
    "    len_topic_info = len(tb_model.get_topic_info())\n",
    "    \n",
    "    tbf_id = f'{l}__{t}'\n",
    "    \n",
    "    for key_train in results_train[tbf_id].keys():\n",
    "        for key_dev in results_dev[tbf_id].keys():\n",
    "\n",
    "            devdict = {}\n",
    "            devdict = substitute_vals(results_dev[tbf_id][key_dev], 'kl',len_topic_info)\n",
    "            distr_dev_general = {k: devdict[k] for k in sorted(devdict)}\n",
    "            distr_dev = [(k, v) for k,v in distr_dev_general.items()]\n",
    "\n",
    "            devdict = {}\n",
    "            devdict = substitute_vals(results_dev[tbf_id][key_dev], 'jaccard',len_topic_info)\n",
    "            distr_dev_jcrd = {k: devdict[k] for k in sorted(devdict)}\n",
    "            distr_dev_jcrd = [(k, v) for k,v in distr_dev_jcrd.items()]\n",
    "\n",
    "\n",
    "            devdict = {}\n",
    "            # print(g, dict(sorted(topic_prop.items(), key=lambda item: item[1])))\n",
    "            devdict = substitute_vals(results_train[tbf_id][key_train], 'kl',len_topic_info)\n",
    "            distr_train_general = {k: devdict[k] for k in sorted(devdict)}\n",
    "            distr_train = [(k, v) for k,v in distr_train_general.items()]\n",
    "\n",
    "            devdict = {}\n",
    "            devdict = substitute_vals(results_train[tbf_id][key_train], 'jaccard',len_topic_info)\n",
    "            distr_train_jcrd = {k: devdict[k] for k in sorted(devdict)}\n",
    "            distr_train_jcrd = [(k, v) for k,v in distr_train_jcrd.items()]\n",
    "\n",
    "\n",
    "            between = f\"{key_train}---{key_dev}\" \n",
    "\n",
    "            # print(tbf_id, between)\n",
    "            hlngr = hellinger(distr_train, distr_dev)\n",
    "            kl = kullback_leibler(distr_train, distr_dev)\n",
    "            jcrd = jaccard(distr_train_jcrd, distr_dev_jcrd)\n",
    "\n",
    "            # print(key_train, key_dev, hlngr, kl, jcrd)\n",
    "            \n",
    "            distances[tbf_id][between] = {'distances': {'kl': kl, 'hlngr': hlngr, 'jcrd': jcrd}}\n",
    "\n",
    "            \n",
    "# uncomment to pickle the distances\n",
    "# with open(f'distances_allmodels.pkl', 'wb') as outp:\n",
    "#     pickle.dump(distances, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8bda13-329c-4b82-8c40-35485544d1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertopic",
   "language": "python",
   "name": "bertopic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
