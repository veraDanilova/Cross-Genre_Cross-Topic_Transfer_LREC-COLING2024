{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a59a5cf-c60c-4c65-95fb-4c1e85ed3185",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "\n",
    "import logging \n",
    "import sys, os, re\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "import pickle\n",
    "import conllu\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from topictuner import TopicModelTuner\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "from gensim.matutils import hellinger\n",
    "from gensim.matutils import kullback_leibler\n",
    "from gensim.matutils import jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc658e3a-cc20-41d9-9dbd-45ae49378e1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# util classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "453d42c3-87d6-402f-9ee5-1a13840a7834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###############\n",
    "#These classes for UD object types (treebank, sentence, word)\n",
    "#are from https://github.com/personads/ud-selection/blob/main/lib/data.py\n",
    "###############\n",
    "class UniversalDependencies:\n",
    "\n",
    "    def __init__(self, treebanks=[]):\n",
    "        self._treebanks = treebanks\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_directory(path, verbose=False):\n",
    "        treebanks = []\n",
    "        cursor = 0\n",
    "            \n",
    "        # iterate over files in TB directory\n",
    "        for tbf in sorted(os.listdir(path)):\n",
    "            tbf_path = os.path.join(path, tbf)\n",
    "            \n",
    "           \n",
    "            # parse TB sample name with lang fam and GMM/gold\n",
    "            r = r'(.+)(?=(\\.dev|\\.train)\\.conllu)'\n",
    "            tb_match = re.match(r, tbf)\n",
    "            if not tb_match:\n",
    "                continue\n",
    "            fname = tb_match.group(0)\n",
    "            # print(f'fname: {fname}\\n')\n",
    "                      \n",
    "            components = fname.split('__')\n",
    "            # print(f'components: {components}\\n')\n",
    "            \n",
    "            tb_meta = defaultdict()\n",
    "            \n",
    "            if ('dev' in tbf_path)&(len(components) == 3):\n",
    "                # tb_meta['Language_family'] = components[0]\n",
    "                tb_meta['Language'] = components[0]\n",
    "                tb_meta['Treebank'] = components[1]\n",
    "                tb_meta['Genre'] = components[2]\n",
    "                \n",
    "            \n",
    "            elif 'train' in tbf_path:\n",
    "                # tb_meta['Language_family'] = components[0]\n",
    "                # tb_meta['Sample_type'] = components[1]\n",
    "                tb_meta['Language'] = components[0]\n",
    "                tb_meta['Treebank'] = components[1]\n",
    "                tb_meta['Genre'] = components[2]\n",
    "                tb_meta['Sample_no'] = components[3]\n",
    "                tb_meta['Seed'] = components[4]\n",
    "            print(tb_meta)    \n",
    "            \n",
    "            # skip non-conllu files\n",
    "            if os.path.splitext(tbf)[1] != '.conllu': continue\n",
    "\n",
    "            # load treebank\n",
    "            treebank = UniversalDependenciesTreebank.from_conllu(tbf_path, name=tbf, meta=tb_meta, start_idx=cursor, ud_filter=None)\n",
    "            treebanks.append(treebank)\n",
    "            cursor += len(treebank)\n",
    "\n",
    "            # print statistics (if verbose)\n",
    "            if verbose:\n",
    "                info = f\"Loaded {treebank}.\"\n",
    "            if logging.getLogger().hasHandlers():\n",
    "                logging.info(info)\n",
    "            else:\n",
    "                print(info)\n",
    "\n",
    "        return UniversalDependencies(treebanks=treebanks)\n",
    "\n",
    "    def get_treebanks(self):\n",
    "        return self._treebanks\n",
    "\n",
    "class UniversalDependenciesTreebank:\n",
    "\tdef __init__(self, sentences=[], name=None, meta={}):\n",
    "\t\tself._sentences = sentences\n",
    "\t\tself._name = name\n",
    "\t\tself._meta = meta\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn f'<UniversalDependenciesTreebank{f\" ({self._name})\" if self._name else \"\"}: {len(self._sentences)} sentences>'\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self._sentences)\n",
    "\n",
    "\tdef __getitem__(self, key):\n",
    "\t\treturn self._sentences[key]\n",
    "\n",
    "\tdef __setitem__(self, key, val):\n",
    "\t\tself._sentences[key] = val\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef from_conllu(path, name=None, meta=None, start_idx=0, ud_filter=None):\n",
    "\t\tsentences = []\n",
    "\t\twith open(path, 'r', encoding='utf8') as fp:\n",
    "\t\t\tcur_lines = []\n",
    "\t\t\tfor line_idx, line in enumerate(fp):\n",
    "\t\t\t\tline = line.strip()\n",
    "\t\t\t\t# on blank line, construct full sentence from preceding lines\n",
    "\t\t\t\tif line == '':\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t# parse sentence from current set of lines\n",
    "\t\t\t\t\t\tsentence = UniversalDependenciesSentence.from_conllu(start_idx + len(sentences), cur_lines)\n",
    "\t\t\t\t\t\t# if filter is set, set any sentences not matching the filter to None\n",
    "\t\t\t\t\t\tif (ud_filter is not None) and (not ud_filter(sentence, meta)): sentence = None\n",
    "\t\t\t\t\t\t# append sentence to results\n",
    "\t\t\t\t\t\tsentences.append(sentence)\n",
    "\t\t\t\t\texcept Exception as err:\n",
    "\t\t\t\t\t\twarn_msg = f\"[Warning] UniversalDependenciesTreebank: Unable to parse '{path}' line {line_idx} ({err}). Skipping.\"\n",
    "\t\t\t\t\t\tif logging.getLogger().hasHandlers():\n",
    "\t\t\t\t\t\t\tlogging.warning(warn_msg)\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tprint(warn_msg)\n",
    "\t\t\t\t\tcur_lines = []\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tcur_lines.append(line)\n",
    "\t\treturn UniversalDependenciesTreebank(sentences=sentences, name=name, meta=meta)\n",
    "\n",
    "\tdef to_tokens(self):\n",
    "\t\tsentences = []\n",
    "\t\tfor sentence in self:\n",
    "\t\t\tsentences.append(sentence.to_tokens())\n",
    "\t\treturn sentences\n",
    "\n",
    "\tdef to_words(self):\n",
    "\t\tsentences = []\n",
    "\t\tfor sentence in self:\n",
    "\t\t\tsentences.append(sentence.to_words())\n",
    "\t\treturn sentences\n",
    "\n",
    "\tdef to_conllu(self, comments=True, resolve=False):\n",
    "\t\treturn ''.join([s.to_conllu(comments, resolve) for s in self._sentences])\n",
    "\n",
    "\tdef get_sentences(self):\n",
    "\t\treturn self._sentences\n",
    "\n",
    "\tdef get_name(self):\n",
    "\t\treturn self._name\n",
    "\n",
    "\tdef get_treebank_name(self):\n",
    "\t\treturn self._meta.get('Treebank', 'Unknown')\n",
    "\n",
    "\tdef get_language(self):\n",
    "\t\treturn self._meta.get('Language', 'Unknown')\n",
    "\n",
    "\tdef get_domains(self):\n",
    "\t\treturn sorted(self._meta.get('Genre', '').split(' '))\n",
    "\n",
    "\tdef get_statistics(self):\n",
    "\t\tstatistics = {\n",
    "\t\t\t'sentences': len(self._sentences),\n",
    "\t\t\t'tokens': 0,\n",
    "\t\t\t'words': 0,\n",
    "\t\t\t'metadata': set()\n",
    "\t\t}\n",
    "\n",
    "\t\tfor sidx, sentence in enumerate(self):\n",
    "\t\t\tstatistics['tokens'] += len(sentence.to_tokens(as_str=False))\n",
    "\t\t\tstatistics['words'] += len(sentence.to_words(as_str=False))\n",
    "\t\t\tstatistics['metadata'] |= set(sentence.get_metadata().keys())\n",
    "\n",
    "\t\tstatistics['metadata'] = list(sorted(statistics['metadata']))\n",
    "\n",
    "\t\treturn statistics\n",
    "\n",
    "    \n",
    "class UniversalDependenciesSentence:\n",
    "\tdef __init__(self, idx, tokens, comments=[]):\n",
    "\t\tself.idx = idx\n",
    "\t\tself._tokens = tokens\n",
    "\t\tself._comments = comments\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn f\"<UniversalDependenciesSentence: ID {self.idx}, {len(self._tokens)} tokens, {len(self._comments)} comments>\"\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef from_conllu(idx, lines):\n",
    "\t\ttokens, comments = [], []\n",
    "\t\tline_idx = 0\n",
    "\t\twhile line_idx < len(lines):\n",
    "\t\t\t# check for comment\n",
    "\t\t\tif lines[line_idx].startswith('#'):\n",
    "\t\t\t\tcomments.append(lines[line_idx])\n",
    "\t\t\t\tline_idx += 1\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t# process tokens\n",
    "\t\t\ttkn_words = []\n",
    "\t\t\ttkn_line_split = lines[line_idx].split('\\t')\n",
    "\t\t\ttkn_idx_str = tkn_line_split[0]\n",
    "\t\t\t# check for multiword token in 'a-b' format\n",
    "\t\t\tnum_words = 1\n",
    "\t\t\tif '-' in tkn_idx_str:\n",
    "\t\t\t\ttkn_idx_split = tkn_idx_str.split('-')\n",
    "\t\t\t\t# convert token id to tuple signifying range (e.g. (3,4))\n",
    "\t\t\t\ttkn_span = (int(tkn_idx_split[0]), int(tkn_idx_split[1]))\n",
    "\t\t\t\t# collect the number of words in the current span\n",
    "\t\t\t\twhile (line_idx + num_words + 1) < len(lines):\n",
    "\t\t\t\t\tnum_words += 1\n",
    "\t\t\t\t\t# get current index as float due to spans such as '1-2; 1; 2; 2.1; ... 3' (e.g. Arabic data)\n",
    "\t\t\t\t\tspan_str = lines[line_idx+num_words].split('\\t')[0]\n",
    "\t\t\t\t\tif '-' in span_str: break\n",
    "\t\t\t\t\tspan_tkn_idx = float(span_str)\n",
    "\t\t\t\t\tif int(span_tkn_idx) > tkn_span[1]: break\n",
    "\t\t\t# check for multiword token in decimal format '1; 1.1; 1.2; ... 2' or '0.1; 0.2; ... 1' (e.g. Czech data)\n",
    "\t\t\telif re.match(r'^\\d+\\.\\d+', tkn_idx_str)\t\t\t\tor ((line_idx < (len(lines) - 1)) and re.match(r'^\\d+\\.\\d+\\t', lines[line_idx+1])):\n",
    "\t\t\t\t# count words that are part of multiword token\n",
    "\t\t\t\twhile (line_idx + num_words) < len(lines):\n",
    "\t\t\t\t\tif not re.match(r'^\\d+\\.\\d+\\t', lines[line_idx+num_words]):\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\tnum_words += 1\n",
    "\t\t\t\t# token span for decimal indices is (a.1, a.n)\n",
    "\t\t\t\ttkn_span_start = float(tkn_idx_str) if re.match(r'^\\d+\\.\\d+', tkn_idx_str) else int(tkn_idx_str) + .1\n",
    "\t\t\t\ttkn_span_end = tkn_span_start + (.1 * (num_words - 1))\n",
    "\t\t\t\ttkn_span = (tkn_span_start, tkn_span_end)\n",
    "\t\t\t# if single word token\n",
    "\t\t\telse:\n",
    "\t\t\t\t# convert token id to tuple with range 1 (e.g. (3,3))\n",
    "\t\t\t\ttkn_span = (int(tkn_idx_str), int(tkn_idx_str))\n",
    "\n",
    "\t\t\t# construct words contained in token\n",
    "\t\t\tfor word_line in lines[line_idx:line_idx + num_words]:\n",
    "\t\t\t\ttkn_words.append(UniversalDependenciesWord.from_conllu(word_line))\n",
    "\t\t\t# construct and append token\n",
    "\t\t\ttokens.append(UniversalDependenciesToken(idx=tkn_span, words=tkn_words))\n",
    "\t\t\t# increment line index by number of words in token\n",
    "\t\t\tline_idx += num_words\n",
    "\n",
    "\t\treturn UniversalDependenciesSentence(idx=idx, tokens=tokens, comments=comments)\n",
    "\n",
    "\tdef to_text(self):\n",
    "\t\treturn ''.join([t.to_text() for t in self._tokens])\n",
    "\n",
    "\tdef to_tokens(self, as_str=True):\n",
    "\t\treturn [(t.get_form() if as_str else t) for t in self._tokens]\n",
    "\n",
    "\tdef to_words(self, as_str=True):\n",
    "\t\treturn [(w.get_form() if as_str else w) for token in self._tokens for w in token.to_words()]\n",
    "\n",
    "\tdef to_conllu(self, comments=True, resolve=False):\n",
    "\t\tconllu = '\\n'.join(self._comments) + '\\n' if comments else ''\n",
    "\n",
    "\t\tconllu += '\\n'.join([t.to_conllu(resolve=resolve) for t in self._tokens])\n",
    "\t\tconllu += '\\n\\n'\n",
    "\n",
    "\t\treturn conllu\n",
    "\n",
    "\tdef get_dependencies(self, offset=-1, include_subtypes=True):\n",
    "\n",
    "\t\tlabels = []\n",
    "\t\theads = []\n",
    "\t\tfor token in self._tokens:\n",
    "\t\t\tfor w in token.to_words():\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\theads.append((w.head + offset))\n",
    "\t\t\t\texcept TypeError:\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\n",
    "\t\tfor token in self._tokens:\n",
    "\t\t\tfor w in token.to_words():\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tif include_subtypes:\n",
    "\t\t\t\t\t\tlabels.append(w.deprel)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tw.deprel.split(':')[0]\n",
    "\t\t\t\texcept TypeError:\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\tif labels[0] == None:       \n",
    "\t\t\tlabels.pop(0)\n",
    "\t\t# heads = [\n",
    "\t\t# \t(w.head + offset)\n",
    "\t\t# \tfor token in self._tokens for w in token.to_words()\n",
    "\t\t# ]\n",
    "\t\t# labels = [\n",
    "\t\t# \tw.deprel if include_subtypes else w.deprel.split(':')[0]\n",
    "\t\t# \tfor token in self._tokens for w in token.to_words()\n",
    "\t\t# ]\n",
    "\t\treturn heads, labels\n",
    "\n",
    "\tdef get_comments(self, stripped=True):\n",
    "\t\treturn [c[1:].strip() for c in self._comments]\n",
    "\n",
    "\tdef get_metadata(self):\n",
    "\t\t\"\"\"Returns metadata from the comments of a sentence.\n",
    "\n",
    "\t\tComment should follow the UD metadata guidelines '# FIELD = VALUE' or '# FIELD VALUE.\n",
    "\t\tLines not following this convention are exported in the 'unknown' field.\n",
    "\n",
    "\t\tReturns a dict of metadata field and value pairs {'FIELD': 'VALUE'}.\n",
    "\t\t\"\"\"\n",
    "\t\tmetadata = {}\n",
    "\t\tmd_patterns = [r'^# ?(.+?) ?= ?(.+)', r'^# ?([^\\s]+?)\\s([^\\s]+)$']\n",
    "\t\tfor comment in self._comments:\n",
    "\t\t\tfor md_pattern in md_patterns:\n",
    "\t\t\t\tmd_match = re.match(md_pattern, comment)\n",
    "\t\t\t\tif md_match:\n",
    "\t\t\t\t\tmetadata[md_match[1]] = md_match[2]\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\telse:\n",
    "\t\t\t\tmetadata['unknown'] = metadata.get('unknown', []) + [comment[1:].strip()]\n",
    "\t\treturn metadata\n",
    "\n",
    "\n",
    "class UniversalDependenciesToken:\n",
    "\tdef __init__(self, idx, words):\n",
    "\t\tself.idx = idx # expects int or float tuple\n",
    "\t\tself._words = words # first element is token form, all following belong to potential multiword tokens\n",
    "\n",
    "\tdef to_text(self):\n",
    "\t\treturn self._words[0].to_text()\n",
    "\n",
    "\tdef to_words(self):\n",
    "\t\t# if single word token\n",
    "\t\tif len(self._words) == 1:\n",
    "\t\t\treturn self._words\n",
    "\t\t# if multiword token\n",
    "\t\telse:\n",
    "\t\t\t# return words which have a dependency head\n",
    "\t\t\treturn [w for w in self._words if w.head is not None]\n",
    "\n",
    "\tdef to_conllu(self, resolve=False):\n",
    "\t\t# resolve multiword tokens into its constituents\n",
    "\t\tif resolve:\n",
    "\t\t\t# if form token has no head (e.g. 'i-j' token), get constituent words\n",
    "\t\t\tif (self._words[0].head is None) and (len(self._words) > 1):\n",
    "\t\t\t\treturn '\\n'.join([w.to_conllu() for w in self._words[1:] if w.head is not None])\n",
    "\t\t\t# if form token has head or it is not a multiword token, return itself\n",
    "\t\t\telif self._words[0].head is not None:\n",
    "\t\t\t\treturn self._words[0].to_conllu()\n",
    "\t\t\t# if token consists of only one word which has no head, omit (e.g. Coptic '0.1')\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn ''\n",
    "\t\t# otherwise return full set of words\n",
    "\t\telse:\n",
    "\t\t\treturn '\\n'.join([w.to_conllu() for w in self._words])\n",
    "\n",
    "\tdef get_form(self):\n",
    "\t\treturn self._words[0].get_form()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "class UniversalDependenciesWord:\n",
    "\t\"\"\"\n",
    "\tID: Word index, integer starting at 1 for each new sentence; may be a range for tokens with multiple words.\n",
    "\tFORM: Word form or punctuation symbol.\n",
    "\tLEMMA: Lemma or stem of word form.\n",
    "\tUPOSTAG: Universal part-of-speech tag drawn from our revised version of the Google universal POS tags.\n",
    "\tXPOSTAG: Language-specific part-of-speech tag; underscore if not available.\n",
    "\tFEATS: List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available.\n",
    "\tHEAD: Head of the current token, which is either a value of ID or zero (0).\n",
    "\tDEPREL: Universal Stanford dependency relation to the HEAD (root iff HEAD = 0) or a defined language-specific subtype of one.\n",
    "\tDEPS: List of secondary dependencies (head-deprel pairs).\n",
    "\tMISC: Any other annotation.\n",
    "\n",
    "\t[1] https://universaldependencies.org/docs/format.html\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, idx, form, lemma, upostag, xpostag, feats, head, deprel, deps=None, misc=None):\n",
    "\t\tself.idx = idx # expects int, float or str\n",
    "\t\tself.form = form\n",
    "\t\tself.lemma = lemma\n",
    "\t\tself.upostag = upostag\n",
    "\t\tself.xpostag = xpostag\n",
    "\t\tself.feats = feats # expects dict\n",
    "\t\tself.head = head # expects int\n",
    "\t\tself.deprel = deprel # expects str\n",
    "\t\tself.deps = deps\n",
    "\t\tself.misc = misc # expects dict\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn f'<UniversalDependenciesWord: ID {self.idx}, \"{self.form}\">'\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef from_conllu(line):\n",
    "\t\t# split line and initially convert '_' values to None\n",
    "\t\tidx_str, form, lemma, upostag, xpostag, feats, head, deprel, deps, misc = [(v if v != '_' else None) for v in line.split('\\t')]\n",
    "\t\t# parse idx string (int 1, decimal 1.1 or string '1-2')\n",
    "\t\tidx = idx_str\n",
    "\t\tif re.match(r'^\\d+\\.\\d+$', idx_str): idx = float(idx_str)\n",
    "\t\telif re.match(r'^\\d+$', idx_str): idx = int(idx_str)\n",
    "\t\t# parse form and lemma (special case '_')\n",
    "\t\tform = form if form is not None else '_'\n",
    "\t\tlemma = lemma if form != '_' else '_'\n",
    "\t\t# parse dependency head idx (int)\n",
    "\t\thead = int(head) if head is not None else head\n",
    "\t\t# parse FEATS dictionaries\n",
    "\t\ttry:\n",
    "\t\t\tfeats = {f.split('=')[0]:f.split('=')[1] for f in feats.split('|')}\n",
    "\t\texcept:\n",
    "\t\t\tfeats = {}\n",
    "\t\t# parse MISC dictionary\n",
    "\t\ttry:\n",
    "\t\t\tmisc = {m.split('=')[0]:m.split('=')[1] for m in misc.split('|')}\n",
    "\t\texcept:\n",
    "\t\t\tmisc = {}\n",
    "\t\t# construct word\n",
    "\t\tword = UniversalDependenciesWord(\n",
    "\t\t\tidx,\n",
    "\t\t\tform, lemma, # form and lemma are str\n",
    "\t\t\tupostag, xpostag, # upostag and xpostag are str\n",
    "\t\t\tfeats,\n",
    "\t\t\thead, deprel, deps, # dependency information as str\n",
    "\t\t\tmisc\n",
    "\t\t)\n",
    "\t\treturn word\n",
    "\n",
    "\tdef to_text(self):\n",
    "\t\ttext = self.get_form() + ' ' # form + space by default\n",
    "\t\t# if 'SpaceAfter=No' remove trailing space\n",
    "\t\tif ('SpaceAfter' in self.misc) and (self.misc['SpaceAfter'] == 'No'):\n",
    "\t\t\ttext = text[:-1]\n",
    "\n",
    "\t\treturn text\n",
    "\n",
    "\tdef to_conllu(self):\n",
    "\t\tconllu = ''\n",
    "\n",
    "\t\t# convert dictionaries\n",
    "\t\tfeats_str = '|'.join([f'{k}={v}' for k, v in sorted(self.feats.items())]) if self.feats else None\n",
    "\t\tmisc_str = '|'.join([f'{k}={v}' for k, v in sorted(self.misc.items())]) if self.misc else None\n",
    "\n",
    "\t\tconllu_values = [\n",
    "\t\t\tstr(self.idx),\n",
    "\t\t\tself.form, self.lemma,\n",
    "\t\t\tself.upostag, self.xpostag,\n",
    "\t\t\tfeats_str,\n",
    "\t\t\tstr(self.head), self.deprel, self.deps,\n",
    "\t\t\tmisc_str\n",
    "\t\t]\n",
    "\t\t# convert None to '_'\n",
    "\t\tconllu_values = [v if v is not None else '_' for v in conllu_values]\n",
    "\n",
    "\t\tconllu = '\\t'.join(conllu_values)\n",
    "\t\treturn conllu\n",
    "\n",
    "\tdef get_form(self):\n",
    "\t\tform = self.form if self.form else ''\n",
    "\t\tform = form.replace('\\xad', '') # sanitize soft hyphens\n",
    "\t\treturn form\n",
    "#initialize comfy functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e9f408-65db-4022-9bfd-c6305308cf52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "# basic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fbe25de-fe89-4e0e-befb-63adef28c7b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "def clean_sentences(sents):\n",
    "        \n",
    "    clean_sents = []\n",
    "    for sent in sents:\n",
    "        sent = re.sub(r'<a_href=\\S+', '', sent)\n",
    "        sent = re.sub(r'<(/)?\\w+>', '', sent)\n",
    "        sent = re.sub(\"[#*]\", \"\", sent)\n",
    "        sent = remove_emoji(sent)\n",
    "        clean_sents.append(sent)\n",
    "        \n",
    "    return clean_sents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743df0de-5111-4ba7-8117-6162813e523d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# BERTOPIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49e4917b-cacf-4f14-92fe-f45422f01e26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class bertopicTM():\n",
    "    \n",
    "    def __init__(self, preprocessed_sentences):\n",
    "        \n",
    "        self.docs = preprocessed_sentences\n",
    "        self.emb_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "        # self.emb_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
    "        \n",
    "        # self.emb_model = SentenceTransformer(\"intfloat/multilingual-e5-base\")\n",
    "        # self.docs = [f'query: {text}' for text in preprocessed_sentences] # multilingual-e5-large requirement  \n",
    "        \n",
    "        self.embeddings : np.ndarray() = None\n",
    "        self.umap_model = UMAP(\n",
    "            n_neighbors=15,\n",
    "            n_components=5,\n",
    "            min_dist=0.0,\n",
    "            metric='cosine',\n",
    "            # low_memory=False,\n",
    "            random_state=42)\n",
    "        self.hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True) #try leaf, eom tends to create big clus\n",
    "        \n",
    "        #tune hdbscan\n",
    "        self.tmt = TopicModelTuner(docs = self.docs, embedding_model = self.emb_model, hdbscan_model=self.hdbscan_model, reducer_model=self.umap_model, reducer_random_state=1337, verbose = 2)\n",
    "        self.BestResultsDF_random = pd.DataFrame()\n",
    "        self.BestResultsDF_pseudoG = pd.DataFrame()\n",
    "        self.BestResultsDF_grid = pd.DataFrame()\n",
    "        \n",
    "        #topic representation\n",
    "        self.vectorizer_model = CountVectorizer(max_df = 0.7, ngram_range = (1,2))\n",
    "        self.ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True, bm25_weighting=True)#bm25_weighting=True - robustness to stopwords\n",
    "        \n",
    "        keybert_model = KeyBERTInspired()\n",
    "        mmr_model = MaximalMarginalRelevance(diversity=0.5)\n",
    "        \n",
    "        self.representation_model = {\n",
    "            \"KeyBERT\": keybert_model,\n",
    "            # \"OpenAI\": openai_model,  # Uncomment if you will use OpenAI\n",
    "            \"MMR\": mmr_model,\n",
    "        }\n",
    "        \n",
    "\n",
    "    def get_range_check(self):\n",
    "\n",
    "        best_num_clusters = max(self.tmt.ResultsDF.sort_values(by = 'number_uncategorized').query('number_uncategorized < 1000').number_of_clusters.tolist())\n",
    "        best_clu_size = self.tmt.ResultsDF.sort_values(by = 'number_uncategorized').query('number_uncategorized < 1000 & number_of_clusters == @best_num_clusters').min_cluster_size.tolist()[0]\n",
    "        if best_clu_size < 30:\n",
    "            perc = 0.5\n",
    "        else:\n",
    "            perc = 0.1\n",
    "        \n",
    "        lower_limit = best_clu_size - int(best_clu_size*perc)\n",
    "        if lower_limit == 1:\n",
    "            lower_limit = 2\n",
    "        higher_limit = best_clu_size + int(best_clu_size*perc)\n",
    "        range_check = [*range(lower_limit,higher_limit)]\n",
    "\n",
    "        return range_check \n",
    "    \n",
    "    def tune_hdbscan(self):\n",
    "        \n",
    "        self.embeddings = self.tmt.createEmbeddings(self.docs)\n",
    "        self.tmt.reduce()\n",
    "        self.tmt.randomSearch([*range(10,15)], [.1, .25, .5, .75, 1], iters = 50)\n",
    "        self.BestResultsDF_random = self.tmt.summarizeResults()\n",
    "        self.BestResultsDF_pseudoG = self.tmt.pseudoGridSearch(self.get_range_check(), [x/100 for x in range(10,101,10)])       \n",
    "        self.BestResultsDF_grid = self.tmt.gridSearch(self.get_range_check())\n",
    "        self.min_clu_size = self.BestResultsDF_grid.query('number_uncategorized == number_uncategorized.min()').min_cluster_size.tolist()[0]\n",
    "        self.min_samples = self.BestResultsDF_grid.query('number_uncategorized == number_uncategorized.min()').min_samples.tolist()[0]#\n",
    "          \n",
    "\n",
    "    def run_TM(self, use_tuning = False, reduce_outliers = False):\n",
    "\n",
    "        if not use_tuning:\n",
    "\n",
    "            hdbscan_model=self.hdbscan_model\n",
    "        \n",
    "        else:\n",
    "\n",
    "            self.tune_hdbscan()\n",
    "\n",
    "            hdbscan_model = HDBSCAN(min_cluster_size=self.min_clu_size, min_samples = self.min_samples, metric='euclidean', cluster_selection_method='eom', prediction_data=True) \n",
    "\n",
    "        # All steps together\n",
    "        model = BERTopic(\n",
    "              embedding_model=self.emb_model,          # Step 1 - Extract embeddings\n",
    "              umap_model=self.umap_model,                    # Step 2 - Reduce dimensionality\n",
    "              hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings\n",
    "              vectorizer_model=self.vectorizer_model,        # Step 4 - Tokenize topics\n",
    "              ctfidf_model=self.ctfidf_model,                # Step 5 - Extract topic words\n",
    "              representation_model=self.representation_model, # Step 6 - (Optional) Fine-tune topic represenations\n",
    "              calculate_probabilities = True,\n",
    "              # top_n_words = 15\n",
    "            )\n",
    "        \n",
    "        self.model = model.fit(self.docs, self.embeddings)\n",
    "        # topics, probs = self.model.transform(self.docs, self.embeddings)\n",
    "        \n",
    "        if reduce_outliers:\n",
    "            \n",
    "            topics_, _ = self.model.transform(self.docs, self.embeddings)\n",
    "            new_topics = self.model.reduce_outliers(self.docs, topics_, strategy=\"embeddings\")\n",
    "            self.model.update_topics(self.docs, topics=new_topics)\n",
    "            \n",
    "            # new_topics = self.model.reduce_outliers(self.docs, self.model.topics_, strategy=\"embeddings\", threshold = 0.5)\n",
    "            # self.model.update_topics(self.docs, topics=new_topics, top_n_words = 15, vectorizer_model=self.vectorizer_model, ctfidf_model=self.ctfidf_model)\n",
    "                          \n",
    "        return self.model\n",
    "        \n",
    "    \n",
    "    def _calculate_topic_diversity(self):\n",
    "        \n",
    "        topic_keywords = self.model.get_topics()\n",
    "\n",
    "        bertopic_topics = []\n",
    "        for k,v in topic_keywords.items():\n",
    "            temp = []\n",
    "            for tup in v:\n",
    "                temp.append(tup[0])\n",
    "            bertopic_topics.append(temp)  \n",
    "\n",
    "        unique_words = set()\n",
    "        for topic in bertopic_topics:\n",
    "            unique_words = unique_words.union(set(topic[:10]))\n",
    "        td = len(unique_words) / (10 * len(bertopic_topics))\n",
    "\n",
    "        return td\n",
    "\n",
    "\n",
    "    def _calculate_cv_npmi(self, docs, topics): \n",
    "\n",
    "        doc = pd.DataFrame({\"Document\": docs,\n",
    "                        \"ID\": range(len(docs)),\n",
    "                        \"Topic\": topics})\n",
    "        documents_per_topic = doc.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "        cleaned_docs = self.model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "        vectorizer = self.model.vectorizer_model\n",
    "        analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "        words = vectorizer.get_feature_names_out()\n",
    "        tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "        dictionary = corpora.Dictionary(tokens)\n",
    "        corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "        topic_words = [[words for words, _ in self.model.get_topic(topic)] \n",
    "                    for topic in range(len(set(topics))-1)]\n",
    "\n",
    "        coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                      texts=tokens, \n",
    "                                      corpus=corpus,\n",
    "                                      dictionary=dictionary, \n",
    "                                      coherence='c_v')\n",
    "        cv_coherence = coherence_model.get_coherence()\n",
    "\n",
    "        coherence_model_npmi = CoherenceModel(topics=topic_words, \n",
    "                                      texts=tokens, \n",
    "                                      corpus=corpus,\n",
    "                                      dictionary=dictionary, \n",
    "                                      coherence='c_npmi')\n",
    "        npmi_coherence = coherence_model_npmi.get_coherence()\n",
    "\n",
    "        return cv_coherence, npmi_coherence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8be169db-c9e2-4615-a673-2cb976ad7e63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def substitute_vals(mean_probs, distance, len_topic_info):\n",
    "    \n",
    "    sorted_dict = {k: mean_probs[k] for k in sorted(mean_probs)}\n",
    "\n",
    "    # Drop key -1 and add keys to complete the range \n",
    "    for key in range(len_topic_info):\n",
    "        if key not in sorted_dict:\n",
    "            if distance == 'jaccard':\n",
    "                sorted_dict[key] = 0.0\n",
    "            else:\n",
    "                sorted_dict[key] = 0.00001\n",
    "                \n",
    "    sorted_dict = {k: sorted_dict[k] for k in sorted(sorted_dict)}\n",
    "    \n",
    "    # Substitute all 0.0 values with 0.0001\n",
    "    for key, value in sorted_dict.items():\n",
    "        if value == 0.0:\n",
    "            if distance == 'jaccard':\n",
    "                sorted_dict[key] = 0.0\n",
    "            else:\n",
    "                sorted_dict[key] = 0.00001       \n",
    "    # try:\n",
    "    #     sorted_dict.pop(-1)\n",
    "    # except:\n",
    "    #     \"NO -1 key\"\n",
    "    return sorted_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a9771b-8ab1-486b-b258-94333a924ce6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LREC-WS test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0990f321-63cb-472f-9848-f235ccf166c5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'fiction'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__fiction.dev.conllu): 500 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'news'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__news.dev.conllu): 636 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'news', 'Sample_no': 's1', 'Seed': 'rs0'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__news__s1__rs0.train.conllu): 881 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'news', 'Sample_no': 's1', 'Seed': 'rs1234'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__news__s1__rs1234.train.conllu): 872 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'news', 'Sample_no': 's1', 'Seed': 'rs42'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__news__s1__rs42.train.conllu): 871 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'news', 'Sample_no': 's2', 'Seed': 'rs0'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__news__s2__rs0.train.conllu): 888 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'news', 'Sample_no': 's2', 'Seed': 'rs1234'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__news__s2__rs1234.train.conllu): 903 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'news', 'Sample_no': 's2', 'Seed': 'rs42'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__news__s2__rs42.train.conllu): 907 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'news', 'Sample_no': 's3', 'Seed': 'rs0'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__news__s3__rs0.train.conllu): 872 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'news', 'Sample_no': 's3', 'Seed': 'rs1234'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__news__s3__rs1234.train.conllu): 929 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'news', 'Sample_no': 's3', 'Seed': 'rs42'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__news__s3__rs42.train.conllu): 885 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'nonfiction_prose'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__nonfiction_prose.dev.conllu): 55 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'social'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__social.dev.conllu): 424 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'social', 'Sample_no': 's1', 'Seed': 'rs0'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__social__s1__rs0.train.conllu): 845 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'social', 'Sample_no': 's1', 'Seed': 'rs1234'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__social__s1__rs1234.train.conllu): 880 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'social', 'Sample_no': 's1', 'Seed': 'rs42'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__social__s1__rs42.train.conllu): 871 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'social', 'Sample_no': 's2', 'Seed': 'rs0'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__social__s2__rs0.train.conllu): 864 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'social', 'Sample_no': 's2', 'Seed': 'rs1234'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__social__s2__rs1234.train.conllu): 910 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'social', 'Sample_no': 's2', 'Seed': 'rs42'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__social__s2__rs42.train.conllu): 868 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'social', 'Sample_no': 's3', 'Seed': 'rs0'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__social__s3__rs0.train.conllu): 885 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'social', 'Sample_no': 's3', 'Seed': 'rs1234'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__social__s3__rs1234.train.conllu): 899 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'social', 'Sample_no': 's3', 'Seed': 'rs42'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__social__s3__rs42.train.conllu): 864 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'wiki'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__wiki.dev.conllu): 212 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'wiki', 'Sample_no': 's1', 'Seed': 'rs0'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__wiki__s1__rs0.train.conllu): 539 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'wiki', 'Sample_no': 's1', 'Seed': 'rs1234'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__wiki__s1__rs1234.train.conllu): 531 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'wiki', 'Sample_no': 's1', 'Seed': 'rs42'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__wiki__s1__rs42.train.conllu): 549 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'wiki', 'Sample_no': 's2', 'Seed': 'rs0'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__wiki__s2__rs0.train.conllu): 546 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'wiki', 'Sample_no': 's2', 'Seed': 'rs1234'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__wiki__s2__rs1234.train.conllu): 551 sentences>.\n",
      "defaultdict(None, {'Language': 'Belarusian', 'Treebank': 'HSE', 'Genre': 'wiki', 'Sample_no': 's2', 'Seed': 'rs42'})\n",
      "Loaded <UniversalDependenciesTreebank (Belarusian__HSE__wiki__s2__rs42.train.conllu): 533 sentences>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-03-23 12:13:24,593 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Belarusian_HSE_tb_model Counter({1: 811, 0: 636, 3: 622, 2: 447, 4: 404, 5: 303, 6: 248, 11: 242, 10: 237, 13: 232, 19: 230, 36: 223, 21: 208, 7: 201, 12: 193, 22: 181, 14: 161, 8: 161, 9: 157, 111: 149, 158: 149, 32: 148, 40: 145, 18: 144, 26: 136, 145: 130, 16: 127, 155: 126, 23: 123, 38: 122, 152: 121, 15: 119, 17: 119, 92: 117, 59: 113, 28: 113, 24: 112, 42: 111, 20: 110, 49: 109, 54: 109, 30: 108, 219: 108, 65: 106, 179: 105, 39: 105, 46: 105, 25: 104, 27: 103, 229: 101, 33: 99, 103: 98, 269: 96, 37: 96, 166: 96, 50: 95, 41: 95, 228: 94, 44: 94, 80: 92, 89: 92, 34: 91, 53: 91, 71: 90, 29: 90, 31: 90, 35: 89, 253: 89, 47: 88, 143: 85, 48: 85, 108: 85, 61: 84, 173: 83, 150: 83, 112: 83, 56: 83, 85: 82, 79: 81, 51: 81, 73: 81, 76: 80, 201: 80, 221: 80, 133: 78, 84: 78, 60: 78, 252: 75, 113: 74, 52: 72, 192: 71, 135: 70, 161: 70, 45: 70, 55: 70, 74: 69, 126: 69, 69: 69, 178: 69, 67: 69, 72: 68, 266: 68, 125: 67, 251: 66, 66: 66, 43: 66, 57: 66, 88: 65, 137: 65, 140: 65, 81: 64, 162: 64, 114: 63, 177: 63, 93: 63, 64: 62, 86: 62, 63: 62, 70: 62, 121: 61, 182: 60, 110: 60, 175: 60, 250: 60, 102: 60, 96: 60, 75: 60, 124: 59, 239: 59, 78: 59, 132: 58, 131: 57, 97: 57, 128: 57, 58: 57, 101: 56, 90: 56, 146: 56, 148: 56, 77: 56, 237: 55, 109: 54, 100: 54, 255: 53, 87: 53, 270: 53, 180: 52, 141: 51, 82: 51, 68: 51, 193: 50, 62: 50, 151: 49, 107: 49, 259: 49, 119: 49, 83: 49, 123: 48, 106: 48, 247: 48, 144: 48, 254: 48, 105: 48, 142: 48, 225: 47, 205: 47, 99: 47, 91: 46, 206: 46, 230: 46, 264: 46, 134: 45, 262: 45, 160: 44, 169: 44, 156: 44, 265: 43, 208: 43, 233: 43, 154: 43, 185: 43, 165: 43, 104: 43, 211: 43, 118: 43, 243: 42, 115: 42, 268: 42, 138: 42, 188: 41, 186: 40, 94: 40, 198: 40, 214: 40, 196: 39, 98: 39, 120: 39, 164: 39, 216: 38, 260: 38, 129: 38, 210: 38, 157: 38, 194: 37, 181: 37, 242: 37, 153: 36, 197: 35, 263: 35, 168: 35, 122: 34, 174: 34, 117: 34, 95: 34, 147: 33, 222: 33, 212: 33, 176: 33, 116: 33, 170: 33, 163: 32, 220: 32, 215: 32, 139: 32, 149: 32, 130: 31, 256: 31, 195: 30, 204: 30, 261: 30, 226: 30, 258: 30, 190: 30, 223: 29, 240: 29, 224: 29, 184: 29, 171: 29, 127: 29, 183: 28, 187: 28, 159: 28, 199: 27, 136: 27, 217: 26, 167: 26, 249: 26, 257: 25, 203: 25, 238: 25, 200: 24, 218: 24, 231: 24, 248: 24, 191: 24, 172: 24, 207: 22, 245: 22, 227: 22, 189: 22, 213: 21, 267: 21, 209: 21, 234: 21, 235: 20, 202: 20, 236: 19, 244: 19, 241: 18, 232: 18, 246: 17, 271: 16})\n",
      "tb_model not clean :  Міліцыянер ужо сядзеў у «шклянцы». \n",
      "— Кульбіцкага? — перапытаў міліцыянер. \n",
      "— Рэвалюцыйная тут недалёка, грамадзянка, — аглядваючы яе, павучальна, тонам настаўніка, адказаў міліцыянер. \n",
      "Так пачалася яго служба на новай пасадзе, служба спакойная і аднастайная, мала чым падобная да вайсковай. \n",
      "Ёй параілі даведацца ў міліцыянера, што стаяў па другі бок плошчы каля круглай шкляной будкі. \n",
      "Вярхоўны суд Беларусі прысудзіў іх да вышэйшай меры пакарання. \n",
      "Канцэрт супраць сьмяротнага пакараньня. \n",
      "Яго і некалькіх пратэстоўцаў затрымалі <a_href=\"https://www.svaboda.org/a/29542581.html\">https://www.svaboda.org/a/29542581.html</a> \n",
      "Іх затрымалі \n",
      "У офісе працуе міліцыя \n",
      "Палітвязьня Дзьмітрыя Паліенку выпусьцілі на волю \n",
      "Кіроўцаў машын з расейскімі нумарамі, якія пабілі менскага маршрутчыка, затрымалі! \n",
      "Нажаль, \"Мола\" адмовілася браць тэкст са згадкай пра штрафы, суткі спрачаліся, ніяк. \n",
      "У 1997-98 гадах была адноўлена крымінальная справа па факце выяўлення ва ўрочышчы Курапаты парэшткаў людзей з прыкметамі гвалтоўнай смерці. \n",
      "Справу даручылі весці Ваеннай пракуратуры. \n",
      "Свабоду палітвязням! \n",
      "<strong>❗️Затрымалі Паўла Севярынца пасля акцыі на Камароўцы! \n",
      "Рэжым, у сваю чаргу, «узнагародзіў» мяне штрафамі агульнай сумай 4575 руб. за тое, што я фактычна выконваў свой абавязак па Канстытуцыі РБ (арт. 57). \n",
      "Па стане на 24.06.2020 рэшта штрафу складае 2965 руб. \n",
      "З апошніх падзей – адбываў адміністрацыйную кару (5 сутак) за ўдзел у легальным(!) пікеце па зборы подпісаў за альтэрнатыўных кандыдатаў на пляцы ля Камароўскага рынку. \n",
      "tbank_model :  Міліцыянер ужо сядзеў у «шклянцы». \n",
      "— Кульбіцкага? — перапытаў міліцыянер. \n",
      "— Рэвалюцыйная тут недалёка, грамадзянка, — аглядваючы яе, павучальна, тонам настаўніка, адказаў міліцыянер. \n",
      "Так пачалася яго служба на новай пасадзе, служба спакойная і аднастайная, мала чым падобная да вайсковай. \n",
      "Ёй параілі даведацца ў міліцыянера, што стаяў па другі бок плошчы каля круглай шкляной будкі. \n",
      "Вярхоўны суд Беларусі прысудзіў іх да вышэйшай меры пакарання. \n",
      "Канцэрт супраць сьмяротнага пакараньня. \n",
      "Яго і некалькіх пратэстоўцаў затрымалі  \n",
      "Іх затрымалі \n",
      "У офісе працуе міліцыя \n",
      "Палітвязьня Дзьмітрыя Паліенку выпусьцілі на волю \n",
      "Кіроўцаў машын з расейскімі нумарамі, якія пабілі менскага маршрутчыка, затрымалі! \n",
      "Нажаль, \"Мола\" адмовілася браць тэкст са згадкай пра штрафы, суткі спрачаліся, ніяк. \n",
      "У 1997-98 гадах была адноўлена крымінальная справа па факце выяўлення ва ўрочышчы Курапаты парэшткаў людзей з прыкметамі гвалтоўнай смерці. \n",
      "Справу даручылі весці Ваеннай пракуратуры. \n",
      "Свабоду палітвязням! \n",
      "Затрымалі Паўла Севярынца пасля акцыі на Камароўцы! \n",
      "Рэжым, у сваю чаргу, «узнагародзіў» мяне штрафамі агульнай сумай 4575 руб. за тое, што я фактычна выконваў свой абавязак па Канстытуцыі РБ (арт. 57). \n",
      "Па стане на 24.06.2020 рэшта штрафу складае 2965 руб. \n",
      "З апошніх падзей – адбываў адміністрацыйную кару (5 сутак) за ўдзел у легальным(!) пікеце па зборы подпісаў за альтэрнатыўных кандыдатаў на пляцы ля Камароўскага рынку. \n",
      "Belarusian__HSE news_1_0---fiction\n",
      "news_1_0 fiction 0.6357742380023563 3.1711369 0.8256276958002271\n",
      "Belarusian__HSE news_1_0---news\n",
      "news_1_0 news 0.3920714735928429 0.9598265 0.6717066798021119\n",
      "Belarusian__HSE news_1_0---nonfiction_prose\n",
      "news_1_0 nonfiction_prose 0.7873923611747282 5.326551 0.9171292952223713\n",
      "Belarusian__HSE news_1_0---social\n",
      "news_1_0 social 0.5798149541391932 1.9794191 0.7778320626218064\n",
      "Belarusian__HSE news_1_0---wiki\n",
      "news_1_0 wiki 0.7824480179224766 4.527094 0.8876892681986595\n",
      "Belarusian__HSE news_1_1234---fiction\n",
      "news_1_1234 fiction 0.64581114370044 3.2648401 0.842894495412844\n",
      "Belarusian__HSE news_1_1234---news\n",
      "news_1_1234 news 0.39455696434608983 1.0709705 0.6738647510241766\n",
      "Belarusian__HSE news_1_1234---nonfiction_prose\n",
      "news_1_1234 nonfiction_prose 0.7773433637686914 5.1952214 0.9123540450375313\n",
      "Belarusian__HSE news_1_1234---social\n",
      "news_1_1234 social 0.552001775764534 1.8505977 0.7575839536091395\n",
      "Belarusian__HSE news_1_1234---wiki\n",
      "news_1_1234 wiki 0.7796586179263839 4.541483 0.8882962610351395\n",
      "Belarusian__HSE news_1_42---fiction\n",
      "news_1_42 fiction 0.6457055619595308 3.271686 0.8360413318025258\n",
      "Belarusian__HSE news_1_42---news\n",
      "news_1_42 news 0.4065195633865057 1.1143341 0.6793490818765392\n",
      "Belarusian__HSE news_1_42---nonfiction_prose\n",
      "news_1_42 nonfiction_prose 0.7757067976652946 5.1949162 0.9110218140068885\n",
      "Belarusian__HSE news_1_42---social\n",
      "news_1_42 social 0.5794609222968086 2.069571 0.7721375885449386\n",
      "Belarusian__HSE news_1_42---wiki\n",
      "news_1_42 wiki 0.7781496287091052 4.452387 0.8777673678053852\n",
      "Belarusian__HSE news_2_0---fiction\n",
      "news_2_0 fiction 0.6462021922634562 3.3512225 0.8438243243243245\n",
      "Belarusian__HSE news_2_0---news\n",
      "news_2_0 news 0.4013627987814548 1.0333309 0.6831017338092813\n",
      "Belarusian__HSE news_2_0---nonfiction_prose\n",
      "news_2_0 nonfiction_prose 0.7882648231870657 5.254108 0.9161036036036037\n",
      "Belarusian__HSE news_2_0---social\n",
      "news_2_0 social 0.5753702177608709 1.9982977 0.7722144314125451\n",
      "Belarusian__HSE news_2_0---wiki\n",
      "news_2_0 wiki 0.7835940647869725 4.6687574 0.8839558898521165\n",
      "Belarusian__HSE news_2_1234---fiction\n",
      "news_2_1234 fiction 0.6393603243290783 3.1870651 0.8276467331118493\n",
      "Belarusian__HSE news_2_1234---news\n",
      "news_2_1234 news 0.4074375580742824 1.0448387 0.6723395808520862\n",
      "Belarusian__HSE news_2_1234---nonfiction_prose\n",
      "news_2_1234 nonfiction_prose 0.7720016907250539 5.170023 0.9080841638981174\n",
      "Belarusian__HSE news_2_1234---social\n",
      "news_2_1234 social 0.5612413863429316 1.9583734 0.7647608077895482\n",
      "Belarusian__HSE news_2_1234---wiki\n",
      "news_2_1234 wiki 0.7741442644901962 4.360162 0.8743470402641094\n",
      "Belarusian__HSE news_2_42---fiction\n",
      "news_2_42 fiction 0.6446150586985842 3.3448615 0.8336824696802648\n",
      "Belarusian__HSE news_2_42---news\n",
      "news_2_42 news 0.40094548203941544 1.0582469 0.6772907088820012\n",
      "Belarusian__HSE news_2_42---nonfiction_prose\n",
      "news_2_42 nonfiction_prose 0.7844740496546477 5.2996597 0.915385386388694\n",
      "Belarusian__HSE news_2_42---social\n",
      "news_2_42 social 0.5830833349291161 1.9698493 0.7801533148883948\n",
      "Belarusian__HSE news_2_42---wiki\n",
      "news_2_42 wiki 0.7876578692632968 4.5826645 0.8867976534709079\n",
      "Belarusian__HSE news_3_0---fiction\n",
      "news_3_0 fiction 0.6455467108533249 3.265995 0.8344816513761466\n",
      "Belarusian__HSE news_3_0---news\n",
      "news_3_0 news 0.3986573174080966 1.0405574 0.6759275287057875\n",
      "Belarusian__HSE news_3_0---nonfiction_prose\n",
      "news_3_0 nonfiction_prose 0.7940523677560318 5.382473 0.9248853211009175\n",
      "Belarusian__HSE news_3_0---social\n",
      "news_3_0 social 0.569304518053467 2.0652575 0.7746992383590097\n",
      "Belarusian__HSE news_3_0---wiki\n",
      "news_3_0 wiki 0.7828493502305387 4.5596566 0.8848558940626624\n",
      "Belarusian__HSE news_3_1234---fiction\n",
      "news_3_1234 fiction 0.6482868227028598 3.2817798 0.8437427341227128\n",
      "Belarusian__HSE news_3_1234---news\n",
      "news_3_1234 news 0.41601172698584776 1.1856349 0.6876891362186976\n",
      "Belarusian__HSE news_3_1234---nonfiction_prose\n",
      "news_3_1234 nonfiction_prose 0.8176193585309366 5.458186 0.9241119483315394\n",
      "Belarusian__HSE news_3_1234---social\n",
      "news_3_1234 social 0.5904013745775079 2.1156282 0.778025164002681\n",
      "Belarusian__HSE news_3_1234---wiki\n",
      "news_3_1234 wiki 0.7861959926104954 4.6458206 0.8803770538416233\n",
      "Belarusian__HSE news_3_42---fiction\n",
      "news_3_42 fiction 0.6237417281660733 3.11628 0.8275310734463273\n",
      "Belarusian__HSE news_3_42---news\n",
      "news_3_42 news 0.40005351597795175 0.96704143 0.6818489500053299\n",
      "Belarusian__HSE news_3_42---nonfiction_prose\n",
      "news_3_42 nonfiction_prose 0.7913471687544483 5.354673 0.9197226502311246\n",
      "Belarusian__HSE news_3_42---social\n",
      "news_3_42 social 0.5835011200053762 2.0643625 0.7737527982091457\n",
      "Belarusian__HSE news_3_42---wiki\n",
      "news_3_42 wiki 0.77800372498115 4.4228125 0.8771239739899795\n",
      "Belarusian__HSE social_1_0---fiction\n",
      "social_1_0 fiction 0.6843786717005143 3.5189624 0.8514260355029586\n",
      "Belarusian__HSE social_1_0---news\n",
      "social_1_0 news 0.5433028081510461 2.1171706 0.750444717353281\n",
      "Belarusian__HSE social_1_0---nonfiction_prose\n",
      "social_1_0 nonfiction_prose 0.7713350097263897 5.0942526 0.91221086605702\n",
      "Belarusian__HSE social_1_0---social\n",
      "social_1_0 social 0.4634127251183669 1.4021701 0.704728145584459\n",
      "Belarusian__HSE social_1_0---wiki\n",
      "social_1_0 wiki 0.7757431852810894 4.5159593 0.8871245952886012\n",
      "Belarusian__HSE social_1_1234---fiction\n",
      "social_1_1234 fiction 0.6789006683394039 3.4387567 0.8562045454545453\n",
      "Belarusian__HSE social_1_1234---news\n",
      "social_1_1234 news 0.5324433460929457 1.9703162 0.7459619782732994\n",
      "Belarusian__HSE social_1_1234---nonfiction_prose\n",
      "social_1_1234 nonfiction_prose 0.7681979188370581 5.096965 0.9119318181818182\n",
      "Belarusian__HSE social_1_1234---social\n",
      "social_1_1234 social 0.468756374814059 1.5768456 0.7178173241852489\n",
      "Belarusian__HSE social_1_1234---wiki\n",
      "social_1_1234 wiki 0.7952362149692472 4.704915 0.8954759862778732\n",
      "Belarusian__HSE social_1_42---fiction\n",
      "social_1_42 fiction 0.6741574733799347 3.3798773 0.8562996555683122\n",
      "Belarusian__HSE social_1_42---news\n",
      "social_1_42 news 0.5385183958225844 2.0719934 0.7440771469214165\n",
      "Belarusian__HSE social_1_42---nonfiction_prose\n",
      "social_1_42 nonfiction_prose 0.7604959466487934 5.1139245 0.9115958668197474\n",
      "Belarusian__HSE social_1_42---social\n",
      "social_1_42 social 0.47562011411964156 1.5686629 0.7234053787665445\n",
      "Belarusian__HSE social_1_42---wiki\n",
      "social_1_42 wiki 0.7851340096074402 4.632116 0.8880867794554079\n",
      "Belarusian__HSE social_2_0---fiction\n",
      "social_2_0 fiction 0.6761497088230823 3.441853 0.860138888888889\n",
      "Belarusian__HSE social_2_0---news\n",
      "social_2_0 news 0.5460324477075706 2.0057368 0.7552956848357799\n",
      "Belarusian__HSE social_2_0---nonfiction_prose\n",
      "social_2_0 nonfiction_prose 0.7655052450491593 5.060727 0.9089015151515152\n",
      "Belarusian__HSE social_2_0---social\n",
      "social_2_0 social 0.46931302178554757 1.4677224 0.7190884870719778\n",
      "Belarusian__HSE social_2_0---wiki\n",
      "social_2_0 wiki 0.7857446446760351 4.702837 0.8884084556254368\n",
      "Belarusian__HSE social_2_1234---fiction\n",
      "social_2_1234 fiction 0.6799861611115819 3.4676032 0.8600329670329669\n",
      "Belarusian__HSE social_2_1234---news\n",
      "social_2_1234 news 0.5367061391796193 2.0064497 0.74893047204368\n",
      "Belarusian__HSE social_2_1234---nonfiction_prose\n",
      "social_2_1234 nonfiction_prose 0.7660210869423204 5.089469 0.9139860139860139\n",
      "Belarusian__HSE social_2_1234---social\n",
      "social_2_1234 social 0.45486167825626067 1.4445682 0.7071635911258554\n",
      "Belarusian__HSE social_2_1234---wiki\n",
      "social_2_1234 wiki 0.7521841277020068 4.3323374 0.8786543645034212\n",
      "Belarusian__HSE social_2_42---fiction\n",
      "social_2_42 fiction 0.6778512990606615 3.32835 0.854258064516129\n",
      "Belarusian__HSE social_2_42---news\n",
      "social_2_42 news 0.5455016648538721 2.0761335 0.748326232501522\n",
      "Belarusian__HSE social_2_42---nonfiction_prose\n",
      "social_2_42 nonfiction_prose 0.7616555719178745 5.080102 0.909813573523251\n",
      "Belarusian__HSE social_2_42---social\n",
      "social_2_42 social 0.44890608761901657 1.3238361 0.7042485436049037\n",
      "Belarusian__HSE social_2_42---wiki\n",
      "social_2_42 wiki 0.7713293212020761 4.4167547 0.8819776541170334\n",
      "Belarusian__HSE social_3_0---fiction\n",
      "social_3_0 fiction 0.672125168371596 3.3987014 0.8588644067796607\n",
      "Belarusian__HSE social_3_0---news\n",
      "social_3_0 news 0.5493027646804693 2.149559 0.7509034218100414\n",
      "Belarusian__HSE social_3_0---nonfiction_prose\n",
      "social_3_0 nonfiction_prose 0.7758499519191804 5.1764627 0.9208525937339495\n",
      "Belarusian__HSE social_3_0---social\n",
      "social_3_0 social 0.47173988783734744 1.539746 0.7159737767828587\n",
      "Belarusian__HSE social_3_0---wiki\n",
      "social_3_0 wiki 0.76277608991142 4.539304 0.8855265963116937\n",
      "Belarusian__HSE social_3_1234---fiction\n",
      "social_3_1234 fiction 0.6844472799302455 3.529896 0.8586974416017799\n",
      "Belarusian__HSE social_3_1234---news\n",
      "social_3_1234 news 0.548874817386922 2.1026707 0.7537331836212152\n",
      "Belarusian__HSE social_3_1234---nonfiction_prose\n",
      "social_3_1234 nonfiction_prose 0.7746154467255933 5.152416 0.9160177975528366\n",
      "Belarusian__HSE social_3_1234---social\n",
      "social_3_1234 social 0.4519381524487784 1.4620175 0.7154700190987894\n",
      "Belarusian__HSE social_3_1234---wiki\n",
      "social_3_1234 wiki 0.7821391181269263 4.6841044 0.8909191554557477\n",
      "Belarusian__HSE social_3_42---fiction\n",
      "social_3_42 fiction 0.6845113823704171 3.5337474 0.8618750000000001\n",
      "Belarusian__HSE social_3_42---news\n",
      "social_3_42 news 0.5627417987279848 2.2249856 0.7592701781970657\n",
      "Belarusian__HSE social_3_42---nonfiction_prose\n",
      "social_3_42 nonfiction_prose 0.7732996418350468 5.0769353 0.9093118686868686\n",
      "Belarusian__HSE social_3_42---social\n",
      "social_3_42 social 0.4798679399413996 1.5807195 0.7192195143256463\n",
      "Belarusian__HSE social_3_42---wiki\n",
      "social_3_42 wiki 0.7845577309572769 4.728078 0.8929070580013977\n",
      "Belarusian__HSE wiki_1_0---fiction\n",
      "wiki_1_0 fiction 0.7496573110782035 4.497962 0.8955714285714285\n",
      "Belarusian__HSE wiki_1_0---news\n",
      "wiki_1_0 news 0.7366719520749755 4.011244 0.8633359003978952\n",
      "Belarusian__HSE wiki_1_0---nonfiction_prose\n",
      "wiki_1_0 nonfiction_prose 0.7926153242103485 5.4160357 0.9192949907235621\n",
      "Belarusian__HSE wiki_1_0---social\n",
      "wiki_1_0 social 0.7205958576731963 3.927129 0.8507893723527149\n",
      "Belarusian__HSE wiki_1_0---wiki\n",
      "wiki_1_0 wiki 0.591320995816902 3.0234861 0.7739830923793191\n",
      "Belarusian__HSE wiki_1_1234---fiction\n",
      "wiki_1_1234 fiction 0.7562868965902911 4.554529 0.8977288135593221\n",
      "Belarusian__HSE wiki_1_1234---news\n",
      "wiki_1_1234 news 0.736403269825186 4.1326623 0.8594188608179657\n",
      "Belarusian__HSE wiki_1_1234---nonfiction_prose\n",
      "wiki_1_1234 nonfiction_prose 0.7970486768910696 5.512397 0.9202876219825372\n",
      "Belarusian__HSE wiki_1_1234---social\n",
      "wiki_1_1234 social 0.7287582083983362 3.8490262 0.8504312795366522\n",
      "Belarusian__HSE wiki_1_1234---wiki\n",
      "wiki_1_1234 wiki 0.5787017361376079 2.8729815 0.7701204562413391\n",
      "Belarusian__HSE wiki_1_42---fiction\n",
      "wiki_1_42 fiction 0.7412642470370066 4.40314 0.8921402550091074\n",
      "Belarusian__HSE wiki_1_42---news\n",
      "wiki_1_42 news 0.7160262819618224 3.8704853 0.8451601539677633\n",
      "Belarusian__HSE wiki_1_42---nonfiction_prose\n",
      "wiki_1_42 nonfiction_prose 0.7871038064556992 5.365103 0.9189600927305845\n",
      "Belarusian__HSE wiki_1_42---social\n",
      "wiki_1_42 social 0.7065799651197142 3.8116994 0.8426878200501773\n",
      "Belarusian__HSE wiki_1_42---wiki\n",
      "wiki_1_42 wiki 0.6036482337176051 3.213496 0.7882427741691587\n",
      "Belarusian__HSE wiki_2_0---fiction\n",
      "wiki_2_0 fiction 0.7597650280516984 4.6592274 0.9024029304029304\n",
      "Belarusian__HSE wiki_2_0---news\n",
      "wiki_2_0 news 0.7282802538225532 4.190585 0.8554927776625892\n",
      "Belarusian__HSE wiki_2_0---nonfiction_prose\n",
      "wiki_2_0 nonfiction_prose 0.7956803063081148 5.595155 0.9304029304029304\n",
      "Belarusian__HSE wiki_2_0---social\n",
      "wiki_2_0 social 0.7087407990459568 3.8155515 0.8463525468242451\n",
      "Belarusian__HSE wiki_2_0---wiki\n",
      "wiki_2_0 wiki 0.6003700118197596 3.1029286 0.7790880503144655\n",
      "Belarusian__HSE wiki_2_1234---fiction\n",
      "wiki_2_1234 fiction 0.7706884673293115 4.672616 0.9008493647912885\n",
      "Belarusian__HSE wiki_2_1234---news\n",
      "wiki_2_1234 news 0.7448669780025181 4.176161 0.8637211930281138\n",
      "Belarusian__HSE wiki_2_1234---nonfiction_prose\n",
      "wiki_2_1234 nonfiction_prose 0.7940936270124027 5.552214 0.9283121597096189\n",
      "Belarusian__HSE wiki_2_1234---social\n",
      "wiki_2_1234 social 0.7152123553863103 3.9751258 0.8531957333150705\n",
      "Belarusian__HSE wiki_2_1234---wiki\n",
      "wiki_2_1234 wiki 0.6090817188050354 3.2775874 0.7943961236859229\n",
      "Belarusian__HSE wiki_2_42---fiction\n",
      "wiki_2_42 fiction 0.7680127107370069 4.79616 0.9044577861163228\n",
      "Belarusian__HSE wiki_2_42---news\n",
      "wiki_2_42 news 0.7586516209781593 4.3769083 0.8748171026703013\n",
      "Belarusian__HSE wiki_2_42---nonfiction_prose\n",
      "wiki_2_42 nonfiction_prose 0.805573777114369 5.669038 0.9324577861163228\n",
      "Belarusian__HSE wiki_2_42---social\n",
      "wiki_2_42 social 0.7194187377662927 3.9474044 0.8521872455662151\n",
      "Belarusian__HSE wiki_2_42---wiki\n",
      "wiki_2_42 wiki 0.5797878400348546 2.9134483 0.7730671882190525\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'academic'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__academic.dev.conllu): 242 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'legal'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__legal.dev.conllu): 204 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'legal', 'Sample_no': 's1', 'Seed': 'rs0'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__legal__s1__rs0.train.conllu): 553 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'legal', 'Sample_no': 's1', 'Seed': 'rs1234'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__legal__s1__rs1234.train.conllu): 559 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'legal', 'Sample_no': 's1', 'Seed': 'rs42'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__legal__s1__rs42.train.conllu): 567 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'legal', 'Sample_no': 's2', 'Seed': 'rs0'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__legal__s2__rs0.train.conllu): 581 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'legal', 'Sample_no': 's2', 'Seed': 'rs1234'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__legal__s2__rs1234.train.conllu): 565 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'legal', 'Sample_no': 's2', 'Seed': 'rs42'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__legal__s2__rs42.train.conllu): 579 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'legal', 'Sample_no': 's3', 'Seed': 'rs0'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__legal__s3__rs0.train.conllu): 603 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'legal', 'Sample_no': 's3', 'Seed': 'rs1234'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__legal__s3__rs1234.train.conllu): 546 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'legal', 'Sample_no': 's3', 'Seed': 'rs42'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__legal__s3__rs42.train.conllu): 544 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'news'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__news.dev.conllu): 157 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'news', 'Sample_no': 's1', 'Seed': 'rs0'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__news__s1__rs0.train.conllu): 540 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'news', 'Sample_no': 's1', 'Seed': 'rs1234'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__news__s1__rs1234.train.conllu): 554 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'news', 'Sample_no': 's1', 'Seed': 'rs42'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__news__s1__rs42.train.conllu): 549 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'news', 'Sample_no': 's2', 'Seed': 'rs0'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__news__s2__rs0.train.conllu): 541 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'news', 'Sample_no': 's2', 'Seed': 'rs1234'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__news__s2__rs1234.train.conllu): 558 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'news', 'Sample_no': 's2', 'Seed': 'rs42'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__news__s2__rs42.train.conllu): 568 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'news', 'Sample_no': 's3', 'Seed': 'rs0'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__news__s3__rs0.train.conllu): 545 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'news', 'Sample_no': 's3', 'Seed': 'rs1234'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__news__s3__rs1234.train.conllu): 516 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'news', 'Sample_no': 's3', 'Seed': 'rs42'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__news__s3__rs42.train.conllu): 523 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'nonfiction', 'Sample_no': 's1', 'Seed': 'rs0'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__nonfiction__s1__rs0.train.conllu): 456 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'nonfiction', 'Sample_no': 's1', 'Seed': 'rs1234'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__nonfiction__s1__rs1234.train.conllu): 458 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'nonfiction', 'Sample_no': 's1', 'Seed': 'rs42'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__nonfiction__s1__rs42.train.conllu): 444 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'nonfiction', 'Sample_no': 's2', 'Seed': 'rs0'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__nonfiction__s2__rs0.train.conllu): 457 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'nonfiction', 'Sample_no': 's2', 'Seed': 'rs1234'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__nonfiction__s2__rs1234.train.conllu): 427 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'nonfiction', 'Sample_no': 's2', 'Seed': 'rs42'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__nonfiction__s2__rs42.train.conllu): 444 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'nonfiction', 'Sample_no': 's3', 'Seed': 'rs0'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__nonfiction__s3__rs0.train.conllu): 457 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'nonfiction', 'Sample_no': 's3', 'Seed': 'rs1234'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__nonfiction__s3__rs1234.train.conllu): 449 sentences>.\n",
      "defaultdict(None, {'Language': 'Czech', 'Treebank': 'CAC', 'Genre': 'nonfiction', 'Sample_no': 's3', 'Seed': 'rs42'})\n",
      "Loaded <UniversalDependenciesTreebank (Czech__CAC__nonfiction__s3__rs42.train.conllu): 438 sentences>.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m     t \u001b[38;5;241m=\u001b[39m tb_set\u001b[38;5;241m.\u001b[39m_meta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTreebank\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     35\u001b[0m tb_sents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(tb_sents) \n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tb_sents) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m20970\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# tb_sents = list(set(tb_sents)) #unique\u001b[39;00m\n\u001b[1;32m     38\u001b[0m clean_docs \u001b[38;5;241m=\u001b[39m clean_sentences(tb_sents)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path = 'LREC_treebanks_test'\n",
    "\n",
    "distances = defaultdict(dict)\n",
    "\n",
    "for tb_dir in sorted(os.listdir(path)):\n",
    "        \n",
    "    ud_path = os.path.join(path, tb_dir)\n",
    "    \n",
    "    if not os.path.isdir(ud_path):\n",
    "        continue\n",
    "       \n",
    "    ud = UniversalDependencies.from_directory(ud_path, verbose=True)\n",
    "    \n",
    "    prev_id = 0    \n",
    "    lengths = defaultdict(dict)\n",
    "    for tb in ud.get_treebanks():\n",
    "        \n",
    "        tb_set_leng = len(tb.get_sentences())\n",
    "        \n",
    "        lengths[tb.get_name()] = {'s' : prev_id, \n",
    "                                  'e': prev_id+tb_set_leng}\n",
    "        prev_id += tb_set_leng\n",
    "            \n",
    "    ################\n",
    "    # Get all sentences in tb and preprocess basically\n",
    "    ################\n",
    "    tb_sents = []\n",
    "    for tb_set in ud.get_treebanks():\n",
    "        \n",
    "        tb_sents+=[s.to_text() for s in tb_set.get_sentences()]\n",
    "        \n",
    "        l = tb_set._meta['Language']\n",
    "        t = tb_set._meta['Treebank']\n",
    "    \n",
    "    tb_sents = list(tb_sents) \n",
    "    # assert len(tb_sents) == 20970 #1st tb\n",
    "    # tb_sents = list(set(tb_sents)) #unique\n",
    "    clean_docs = clean_sentences(tb_sents)\n",
    "    \n",
    "    ################\n",
    "    BT = bertopicTM(clean_docs)\n",
    "    tb_model = BT.run_TM(use_tuning = False, reduce_outliers = True)\n",
    "        \n",
    "    \n",
    "#     print(f'{l}_{t}_tb_model', tb_model.topic_sizes_)\n",
    "#     spectopic_indices = [n for n, tid in enumerate(tb_model.topics_) if tid == 0]\n",
    "#     print('tb_model_sample_sentences: ','\\n'.join([s for n, s in enumerate(clean_docs) if n in spectopic_indices][:20]))\n",
    "    \n",
    "    # if (l == 'English')&(t == 'GUM'):\n",
    "    #     tbank_model.save(f\"{l}_{t}_model\", serialization=\"safetensors\")\n",
    "    \n",
    "    results_dev = defaultdict(dict)\n",
    "\n",
    "    for k in lengths.keys():\n",
    "        if 'dev' in k:\n",
    "            \n",
    "            reg = re.match(r'^([a-zA-Z-]+)__([a-zA-Z-]+)__([a-zA-Z_-]+).dev.conllu', k)\n",
    "            l = reg.group(1)\n",
    "            cor = reg.group(2)\n",
    "            g = reg.group(3)\n",
    "            \n",
    "            topic_prop = {key: value / sum(Counter(tb_model.topics_[lengths[k]['s']: lengths[k]['e']]).values()) \n",
    "                          for key, value in Counter(tb_model.topics_[lengths[k]['s']: lengths[k]['e']]).items()}\n",
    "            #dict(sorted(topic_prop.items(), key=lambda item: item[1]))\n",
    "            results_dev[f'{l}__{cor}'][f'{g}'] = topic_prop\n",
    "    \n",
    "    results_train = defaultdict(dict)\n",
    "\n",
    "    for k in lengths.keys():\n",
    "        if 'train' in k:\n",
    "            \n",
    "            reg = re.match(r'^([a-zA-Z-]+)__([a-zA-Z-]+)__([a-zA-Z_-]+)__s(\\d)__rs(\\d+).train.conllu', k)\n",
    "            l = reg.group(1)\n",
    "            cor = reg.group(2)\n",
    "            g = reg.group(3)\n",
    "            samp = reg.group(4)\n",
    "            rseed = reg.group(5)\n",
    "            \n",
    "            topic_prop = {}\n",
    "            topic_prop = {key: value / sum(Counter(tb_model.topics_[lengths[k]['s']: lengths[k]['e']]).values())\n",
    "                          for key, value in Counter(tb_model.topics_[lengths[k]['s']: lengths[k]['e']]).items()}\n",
    "\n",
    "            results_train[f'{l}__{cor}'][f'{g}_{samp}_{rseed}'] = topic_prop\n",
    "    \n",
    "    len_topic_info = len(tb_model.get_topic_info())\n",
    "    \n",
    "    tbf_id = f'{l}__{t}'\n",
    "    \n",
    "    for key_train in results_train[tbf_id].keys():\n",
    "        for key_dev in results_dev[tbf_id].keys():\n",
    "\n",
    "            devdict = {}\n",
    "            devdict = substitute_vals(results_dev[tbf_id][key_dev], 'kl',len_topic_info)\n",
    "            distr_dev_general = {k: devdict[k] for k in sorted(devdict)}\n",
    "            distr_dev = [(k, v) for k,v in distr_dev_general.items()]\n",
    "\n",
    "            devdict = {}\n",
    "            devdict = substitute_vals(results_dev[tbf_id][key_dev], 'jaccard',len_topic_info)\n",
    "            distr_dev_jcrd = {k: devdict[k] for k in sorted(devdict)}\n",
    "            distr_dev_jcrd = [(k, v) for k,v in distr_dev_jcrd.items()]\n",
    "\n",
    "\n",
    "            devdict = {}\n",
    "            # print(g, dict(sorted(topic_prop.items(), key=lambda item: item[1])))\n",
    "            devdict = substitute_vals(results_train[tbf_id][key_train], 'kl',len_topic_info)\n",
    "            distr_train_general = {k: devdict[k] for k in sorted(devdict)}\n",
    "            distr_train = [(k, v) for k,v in distr_train_general.items()]\n",
    "\n",
    "            devdict = {}\n",
    "            devdict = substitute_vals(results_train[tbf_id][key_train], 'jaccard',len_topic_info)\n",
    "            distr_train_jcrd = {k: devdict[k] for k in sorted(devdict)}\n",
    "            distr_train_jcrd = [(k, v) for k,v in distr_train_jcrd.items()]\n",
    "\n",
    "\n",
    "            between = f\"{key_train}---{key_dev}\" \n",
    "\n",
    "            # print(tbf_id, between)\n",
    "            hlngr = hellinger(distr_train, distr_dev)\n",
    "            kl = kullback_leibler(distr_train, distr_dev)\n",
    "            jcrd = jaccard(distr_train_jcrd, distr_dev_jcrd)\n",
    "\n",
    "            # print(key_train, key_dev, hlngr, kl, jcrd)\n",
    "            \n",
    "            distances[tbf_id][between] = {'distances': {'kl': kl, 'hlngr': hlngr, 'jcrd': jcrd}}\n",
    "\n",
    "            \n",
    "# uncomment to pickle the distances\n",
    "# with open(f'distances_allmodels.pkl', 'wb') as outp:\n",
    "#     pickle.dump(distances, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8bda13-329c-4b82-8c40-35485544d1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertopic",
   "language": "python",
   "name": "bertopic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
